{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits.\n",
    "* Apply L1 and L2 regularization.\n",
    "* Aplly dropout regularization.\n",
    "* Observe and comment on the effect of using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'tail'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9eebe5ec8e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'tail'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Your code here\n",
    "# X_train = \n",
    "# X_test = \n",
    "# y_train = \n",
    "# y_test = \n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.9385 - acc: 0.1769 - val_loss: 1.9247 - val_acc: 0.2050\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9219 - acc: 0.1939 - val_loss: 1.9076 - val_acc: 0.2320\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9041 - acc: 0.2064 - val_loss: 1.8890 - val_acc: 0.2370\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8847 - acc: 0.2192 - val_loss: 1.8685 - val_acc: 0.2450\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8633 - acc: 0.2296 - val_loss: 1.8455 - val_acc: 0.2600\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8390 - acc: 0.2475 - val_loss: 1.8192 - val_acc: 0.2770\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8117 - acc: 0.2668 - val_loss: 1.7901 - val_acc: 0.3090\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7813 - acc: 0.2985 - val_loss: 1.7568 - val_acc: 0.3460\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7467 - acc: 0.3268 - val_loss: 1.7209 - val_acc: 0.3780\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7078 - acc: 0.3667 - val_loss: 1.6792 - val_acc: 0.3990\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6638 - acc: 0.4004 - val_loss: 1.6348 - val_acc: 0.4510\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6149 - acc: 0.4501 - val_loss: 1.5847 - val_acc: 0.4870\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5628 - acc: 0.4861 - val_loss: 1.5321 - val_acc: 0.5190\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5098 - acc: 0.5205 - val_loss: 1.4790 - val_acc: 0.5430\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.4566 - acc: 0.5476 - val_loss: 1.4262 - val_acc: 0.5820\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4037 - acc: 0.5777 - val_loss: 1.3734 - val_acc: 0.5980\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3515 - acc: 0.6000 - val_loss: 1.3254 - val_acc: 0.6290\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3009 - acc: 0.6272 - val_loss: 1.2743 - val_acc: 0.6360\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2517 - acc: 0.6407 - val_loss: 1.2255 - val_acc: 0.6530\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2045 - acc: 0.6576 - val_loss: 1.1822 - val_acc: 0.6710\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1602 - acc: 0.6711 - val_loss: 1.1414 - val_acc: 0.6700\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1180 - acc: 0.6768 - val_loss: 1.1014 - val_acc: 0.6850\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0785 - acc: 0.6901 - val_loss: 1.0663 - val_acc: 0.6900\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0414 - acc: 0.6972 - val_loss: 1.0333 - val_acc: 0.6950\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0072 - acc: 0.7029 - val_loss: 1.0004 - val_acc: 0.7000\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9757 - acc: 0.7071 - val_loss: 0.9723 - val_acc: 0.7050\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9462 - acc: 0.7136 - val_loss: 0.9475 - val_acc: 0.7040\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9192 - acc: 0.7164 - val_loss: 0.9242 - val_acc: 0.7040\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8938 - acc: 0.7233 - val_loss: 0.9024 - val_acc: 0.7110\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8707 - acc: 0.7281 - val_loss: 0.8861 - val_acc: 0.7120\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8495 - acc: 0.7321 - val_loss: 0.8647 - val_acc: 0.7160\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8292 - acc: 0.7375 - val_loss: 0.8501 - val_acc: 0.7140\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8107 - acc: 0.7400 - val_loss: 0.8366 - val_acc: 0.7100\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7933 - acc: 0.7423 - val_loss: 0.8218 - val_acc: 0.7150\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7778 - acc: 0.7460 - val_loss: 0.8099 - val_acc: 0.7140\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7628 - acc: 0.7471 - val_loss: 0.7972 - val_acc: 0.7180\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7485 - acc: 0.7525 - val_loss: 0.7852 - val_acc: 0.7190\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7351 - acc: 0.7539 - val_loss: 0.7761 - val_acc: 0.7180\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7227 - acc: 0.7557 - val_loss: 0.7715 - val_acc: 0.7190\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7112 - acc: 0.7584 - val_loss: 0.7582 - val_acc: 0.7170\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7002 - acc: 0.7616 - val_loss: 0.7510 - val_acc: 0.7190\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6897 - acc: 0.7648 - val_loss: 0.7463 - val_acc: 0.7240\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6798 - acc: 0.7651 - val_loss: 0.7395 - val_acc: 0.7210\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6703 - acc: 0.7673 - val_loss: 0.7294 - val_acc: 0.7280\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6615 - acc: 0.7689 - val_loss: 0.7280 - val_acc: 0.7320\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6526 - acc: 0.7720 - val_loss: 0.7188 - val_acc: 0.7280\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.6447 - acc: 0.7735 - val_loss: 0.7206 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6365 - acc: 0.7793 - val_loss: 0.7180 - val_acc: 0.7300\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6295 - acc: 0.7793 - val_loss: 0.7149 - val_acc: 0.7260\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6216 - acc: 0.7836 - val_loss: 0.7014 - val_acc: 0.7440\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6149 - acc: 0.7833 - val_loss: 0.6981 - val_acc: 0.7370\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6078 - acc: 0.7885 - val_loss: 0.6991 - val_acc: 0.7340\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6011 - acc: 0.7905 - val_loss: 0.6933 - val_acc: 0.7380\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5949 - acc: 0.7897 - val_loss: 0.6881 - val_acc: 0.7350\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5888 - acc: 0.7955 - val_loss: 0.6891 - val_acc: 0.7370\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5823 - acc: 0.7959 - val_loss: 0.6899 - val_acc: 0.7380\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5767 - acc: 0.7980 - val_loss: 0.6857 - val_acc: 0.7370\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5714 - acc: 0.7991 - val_loss: 0.6815 - val_acc: 0.7400\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5659 - acc: 0.8008 - val_loss: 0.6771 - val_acc: 0.7440\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5608 - acc: 0.8031 - val_loss: 0.6816 - val_acc: 0.7390\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5555 - acc: 0.8028 - val_loss: 0.6767 - val_acc: 0.7390\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5503 - acc: 0.8068 - val_loss: 0.6715 - val_acc: 0.7460\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5455 - acc: 0.8080 - val_loss: 0.6699 - val_acc: 0.7470\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5402 - acc: 0.8096 - val_loss: 0.6698 - val_acc: 0.7440\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5354 - acc: 0.8116 - val_loss: 0.6676 - val_acc: 0.7470\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5308 - acc: 0.8125 - val_loss: 0.6680 - val_acc: 0.7430\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5264 - acc: 0.8159 - val_loss: 0.6678 - val_acc: 0.7460\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5221 - acc: 0.8153 - val_loss: 0.6605 - val_acc: 0.7580\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5172 - acc: 0.8189 - val_loss: 0.6592 - val_acc: 0.7540\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5129 - acc: 0.8196 - val_loss: 0.6592 - val_acc: 0.7580\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5087 - acc: 0.8224 - val_loss: 0.6594 - val_acc: 0.7600\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5045 - acc: 0.8261 - val_loss: 0.6539 - val_acc: 0.7580\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5011 - acc: 0.8251 - val_loss: 0.6559 - val_acc: 0.7570\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4964 - acc: 0.8264 - val_loss: 0.6617 - val_acc: 0.7580\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4926 - acc: 0.8297 - val_loss: 0.6567 - val_acc: 0.7580\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4886 - acc: 0.8308 - val_loss: 0.6579 - val_acc: 0.7550\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4846 - acc: 0.8299 - val_loss: 0.6504 - val_acc: 0.7620\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4806 - acc: 0.8337 - val_loss: 0.6590 - val_acc: 0.7570\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4770 - acc: 0.8347 - val_loss: 0.6532 - val_acc: 0.7610\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4734 - acc: 0.8373 - val_loss: 0.6524 - val_acc: 0.7610\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4697 - acc: 0.8401 - val_loss: 0.6519 - val_acc: 0.7580\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4664 - acc: 0.8393 - val_loss: 0.6482 - val_acc: 0.7630\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4628 - acc: 0.8420 - val_loss: 0.6512 - val_acc: 0.7570\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4590 - acc: 0.8412 - val_loss: 0.6510 - val_acc: 0.7620\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4555 - acc: 0.8455 - val_loss: 0.6499 - val_acc: 0.7630\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4524 - acc: 0.8471 - val_loss: 0.6522 - val_acc: 0.7600\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4486 - acc: 0.8476 - val_loss: 0.6477 - val_acc: 0.7630\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4456 - acc: 0.8480 - val_loss: 0.6508 - val_acc: 0.7650\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4420 - acc: 0.8493 - val_loss: 0.6481 - val_acc: 0.7650\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4389 - acc: 0.8519 - val_loss: 0.6466 - val_acc: 0.7640\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4353 - acc: 0.8543 - val_loss: 0.6469 - val_acc: 0.7670\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4321 - acc: 0.8547 - val_loss: 0.6441 - val_acc: 0.7640\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4294 - acc: 0.8564 - val_loss: 0.6445 - val_acc: 0.7680\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4259 - acc: 0.8579 - val_loss: 0.6501 - val_acc: 0.7650\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.4231 - acc: 0.8587 - val_loss: 0.6464 - val_acc: 0.7650\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4193 - acc: 0.8624 - val_loss: 0.6427 - val_acc: 0.7690\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4165 - acc: 0.8607 - val_loss: 0.6410 - val_acc: 0.7650\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4133 - acc: 0.8624 - val_loss: 0.6488 - val_acc: 0.7680\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4106 - acc: 0.8639 - val_loss: 0.6430 - val_acc: 0.7690\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4073 - acc: 0.8640 - val_loss: 0.6440 - val_acc: 0.7680\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4046 - acc: 0.8665 - val_loss: 0.6491 - val_acc: 0.7720\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4018 - acc: 0.8671 - val_loss: 0.6433 - val_acc: 0.7680\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3985 - acc: 0.8684 - val_loss: 0.6467 - val_acc: 0.7650\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3961 - acc: 0.8688 - val_loss: 0.6518 - val_acc: 0.7670\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3934 - acc: 0.8695 - val_loss: 0.6457 - val_acc: 0.7670\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3906 - acc: 0.8725 - val_loss: 0.6482 - val_acc: 0.7640\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3874 - acc: 0.8727 - val_loss: 0.6451 - val_acc: 0.7670\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3852 - acc: 0.8744 - val_loss: 0.6454 - val_acc: 0.7640\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3822 - acc: 0.8759 - val_loss: 0.6466 - val_acc: 0.7650\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3801 - acc: 0.8751 - val_loss: 0.6469 - val_acc: 0.7650\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3771 - acc: 0.8789 - val_loss: 0.6425 - val_acc: 0.7690\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3741 - acc: 0.8781 - val_loss: 0.6526 - val_acc: 0.7620\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3721 - acc: 0.8779 - val_loss: 0.6458 - val_acc: 0.7650\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3692 - acc: 0.8801 - val_loss: 0.6449 - val_acc: 0.7680\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3665 - acc: 0.8820 - val_loss: 0.6448 - val_acc: 0.7670\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3640 - acc: 0.8807 - val_loss: 0.6457 - val_acc: 0.7680\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3612 - acc: 0.8825 - val_loss: 0.6466 - val_acc: 0.7700\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3589 - acc: 0.8836 - val_loss: 0.6522 - val_acc: 0.7700\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3564 - acc: 0.8841 - val_loss: 0.6485 - val_acc: 0.7630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3542 - acc: 0.8851 - val_loss: 0.6485 - val_acc: 0.7660\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 64us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 99us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3506971211274465, 0.8886666666984558]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6145947367350261, 0.7746666661898295]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.9398 - acc: 0.1720 - val_loss: 1.9321 - val_acc: 0.1850\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9210 - acc: 0.1887 - val_loss: 1.9140 - val_acc: 0.1970\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9033 - acc: 0.2027 - val_loss: 1.8950 - val_acc: 0.2080\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8835 - acc: 0.2188 - val_loss: 1.8724 - val_acc: 0.2330\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8601 - acc: 0.2340 - val_loss: 1.8457 - val_acc: 0.2500\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8323 - acc: 0.2591 - val_loss: 1.8141 - val_acc: 0.2780\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8004 - acc: 0.2840 - val_loss: 1.7800 - val_acc: 0.3130\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7650 - acc: 0.3153 - val_loss: 1.7427 - val_acc: 0.3380\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7266 - acc: 0.3435 - val_loss: 1.7030 - val_acc: 0.3650\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6863 - acc: 0.3741 - val_loss: 1.6606 - val_acc: 0.3810\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6448 - acc: 0.4032 - val_loss: 1.6181 - val_acc: 0.4100\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6027 - acc: 0.4245 - val_loss: 1.5766 - val_acc: 0.4350\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5600 - acc: 0.4516 - val_loss: 1.5341 - val_acc: 0.4540\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5175 - acc: 0.4763 - val_loss: 1.4915 - val_acc: 0.4710\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4752 - acc: 0.5000 - val_loss: 1.4512 - val_acc: 0.4930\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4337 - acc: 0.5184 - val_loss: 1.4123 - val_acc: 0.5170\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3929 - acc: 0.5409 - val_loss: 1.3714 - val_acc: 0.5350\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3522 - acc: 0.5579 - val_loss: 1.3330 - val_acc: 0.5520\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3126 - acc: 0.5748 - val_loss: 1.2965 - val_acc: 0.5690\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.2746 - acc: 0.5916 - val_loss: 1.2598 - val_acc: 0.5780\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2375 - acc: 0.6045 - val_loss: 1.2267 - val_acc: 0.5840\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2018 - acc: 0.6172 - val_loss: 1.1950 - val_acc: 0.5980\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1681 - acc: 0.6276 - val_loss: 1.1629 - val_acc: 0.6040\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1357 - acc: 0.6361 - val_loss: 1.1335 - val_acc: 0.6100\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1050 - acc: 0.6461 - val_loss: 1.1098 - val_acc: 0.6240\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0760 - acc: 0.6548 - val_loss: 1.0815 - val_acc: 0.6250\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0485 - acc: 0.6636 - val_loss: 1.0574 - val_acc: 0.6310\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0229 - acc: 0.6693 - val_loss: 1.0360 - val_acc: 0.6370\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9985 - acc: 0.6765 - val_loss: 1.0161 - val_acc: 0.6460\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9757 - acc: 0.6828 - val_loss: 0.9963 - val_acc: 0.6460\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9541 - acc: 0.6900 - val_loss: 0.9810 - val_acc: 0.6560\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9340 - acc: 0.6948 - val_loss: 0.9620 - val_acc: 0.6520\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9147 - acc: 0.6985 - val_loss: 0.9474 - val_acc: 0.6530\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8973 - acc: 0.7056 - val_loss: 0.9318 - val_acc: 0.6600\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8795 - acc: 0.7100 - val_loss: 0.9180 - val_acc: 0.6610\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8635 - acc: 0.7163 - val_loss: 0.9060 - val_acc: 0.6680\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8480 - acc: 0.7176 - val_loss: 0.8945 - val_acc: 0.6790\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8335 - acc: 0.7251 - val_loss: 0.8851 - val_acc: 0.6790\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8193 - acc: 0.7261 - val_loss: 0.8731 - val_acc: 0.6840\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8060 - acc: 0.7299 - val_loss: 0.8653 - val_acc: 0.6730\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7936 - acc: 0.7327 - val_loss: 0.8544 - val_acc: 0.6870\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7813 - acc: 0.7360 - val_loss: 0.8461 - val_acc: 0.6900\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7694 - acc: 0.7411 - val_loss: 0.8397 - val_acc: 0.6900\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7587 - acc: 0.7416 - val_loss: 0.8299 - val_acc: 0.6910\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7481 - acc: 0.7467 - val_loss: 0.8239 - val_acc: 0.6960\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7376 - acc: 0.7507 - val_loss: 0.8188 - val_acc: 0.6880\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7283 - acc: 0.7503 - val_loss: 0.8105 - val_acc: 0.6990\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7189 - acc: 0.7567 - val_loss: 0.8071 - val_acc: 0.6970\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7101 - acc: 0.7599 - val_loss: 0.8000 - val_acc: 0.6960\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7016 - acc: 0.7611 - val_loss: 0.7919 - val_acc: 0.7020\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6935 - acc: 0.7653 - val_loss: 0.7904 - val_acc: 0.6990\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6854 - acc: 0.7657 - val_loss: 0.7827 - val_acc: 0.7000\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.6775 - acc: 0.7700 - val_loss: 0.7792 - val_acc: 0.7100\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6702 - acc: 0.7728 - val_loss: 0.7736 - val_acc: 0.7100\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6635 - acc: 0.7739 - val_loss: 0.7687 - val_acc: 0.7120\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6558 - acc: 0.7772 - val_loss: 0.7654 - val_acc: 0.7060\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6489 - acc: 0.7793 - val_loss: 0.7607 - val_acc: 0.7200\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6427 - acc: 0.7821 - val_loss: 0.7580 - val_acc: 0.7230\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6359 - acc: 0.7820 - val_loss: 0.7527 - val_acc: 0.7220\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6298 - acc: 0.7849 - val_loss: 0.7548 - val_acc: 0.7090\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5689828497727712, 0.8097333333651224]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7319343857765198, 0.7146666668256124]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 2.6233 - acc: 0.1453 - val_loss: 2.6119 - val_acc: 0.1410\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.6022 - acc: 0.1607 - val_loss: 2.5969 - val_acc: 0.1550\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.5883 - acc: 0.1715 - val_loss: 2.5843 - val_acc: 0.1820\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.5759 - acc: 0.1840 - val_loss: 2.5722 - val_acc: 0.1880\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.5636 - acc: 0.2016 - val_loss: 2.5599 - val_acc: 0.2180\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.5509 - acc: 0.2193 - val_loss: 2.5465 - val_acc: 0.2320\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.5370 - acc: 0.2423 - val_loss: 2.5313 - val_acc: 0.2480\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.5212 - acc: 0.2653 - val_loss: 2.5140 - val_acc: 0.2800\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.5030 - acc: 0.2847 - val_loss: 2.4945 - val_acc: 0.3050\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.4821 - acc: 0.3128 - val_loss: 2.4725 - val_acc: 0.3210\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.4583 - acc: 0.3372 - val_loss: 2.4480 - val_acc: 0.3480\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.4311 - acc: 0.3620 - val_loss: 2.4194 - val_acc: 0.3810\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.4000 - acc: 0.3900 - val_loss: 2.3875 - val_acc: 0.4030\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.3645 - acc: 0.4213 - val_loss: 2.3504 - val_acc: 0.4350\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.3244 - acc: 0.4561 - val_loss: 2.3079 - val_acc: 0.4590\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.2794 - acc: 0.4860 - val_loss: 2.2602 - val_acc: 0.4860\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2303 - acc: 0.5095 - val_loss: 2.2096 - val_acc: 0.5100\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.1791 - acc: 0.5356 - val_loss: 2.1571 - val_acc: 0.5340\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.1262 - acc: 0.5572 - val_loss: 2.1037 - val_acc: 0.5520\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.0726 - acc: 0.5755 - val_loss: 2.0498 - val_acc: 0.5760\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.0187 - acc: 0.5923 - val_loss: 1.9966 - val_acc: 0.5920\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9658 - acc: 0.6072 - val_loss: 1.9449 - val_acc: 0.6110\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9146 - acc: 0.6260 - val_loss: 1.8947 - val_acc: 0.6270\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8655 - acc: 0.6372 - val_loss: 1.8487 - val_acc: 0.6420\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8194 - acc: 0.6525 - val_loss: 1.8054 - val_acc: 0.6430\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7759 - acc: 0.6589 - val_loss: 1.7638 - val_acc: 0.6520\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7353 - acc: 0.6708 - val_loss: 1.7248 - val_acc: 0.6590\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6970 - acc: 0.6759 - val_loss: 1.6895 - val_acc: 0.6720\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6614 - acc: 0.6845 - val_loss: 1.6585 - val_acc: 0.6760\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6281 - acc: 0.6967 - val_loss: 1.6265 - val_acc: 0.6780\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5970 - acc: 0.7001 - val_loss: 1.5982 - val_acc: 0.6870\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5679 - acc: 0.7059 - val_loss: 1.5711 - val_acc: 0.6890\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5407 - acc: 0.7120 - val_loss: 1.5468 - val_acc: 0.6940\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5150 - acc: 0.7156 - val_loss: 1.5244 - val_acc: 0.6930\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4909 - acc: 0.7201 - val_loss: 1.5044 - val_acc: 0.6970\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4683 - acc: 0.7239 - val_loss: 1.4833 - val_acc: 0.6970\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4471 - acc: 0.7265 - val_loss: 1.4661 - val_acc: 0.7060\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4270 - acc: 0.7307 - val_loss: 1.4498 - val_acc: 0.7040\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4083 - acc: 0.7327 - val_loss: 1.4311 - val_acc: 0.7130\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3904 - acc: 0.7360 - val_loss: 1.4162 - val_acc: 0.7140\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3738 - acc: 0.7373 - val_loss: 1.4023 - val_acc: 0.7100\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3576 - acc: 0.7412 - val_loss: 1.3910 - val_acc: 0.7070\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3425 - acc: 0.7441 - val_loss: 1.3782 - val_acc: 0.7160\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3281 - acc: 0.7461 - val_loss: 1.3662 - val_acc: 0.7120\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3144 - acc: 0.7501 - val_loss: 1.3549 - val_acc: 0.7200\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3021 - acc: 0.7520 - val_loss: 1.3467 - val_acc: 0.7180\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2895 - acc: 0.7548 - val_loss: 1.3369 - val_acc: 0.7230\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2777 - acc: 0.7592 - val_loss: 1.3284 - val_acc: 0.7190\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2664 - acc: 0.7601 - val_loss: 1.3191 - val_acc: 0.7210\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2557 - acc: 0.7601 - val_loss: 1.3103 - val_acc: 0.7260\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2455 - acc: 0.7637 - val_loss: 1.3051 - val_acc: 0.7270\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2357 - acc: 0.7671 - val_loss: 1.2946 - val_acc: 0.7250\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2258 - acc: 0.7668 - val_loss: 1.2881 - val_acc: 0.7240\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2168 - acc: 0.7680 - val_loss: 1.2796 - val_acc: 0.7330\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2077 - acc: 0.7708 - val_loss: 1.2745 - val_acc: 0.7280\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1992 - acc: 0.7724 - val_loss: 1.2703 - val_acc: 0.7340\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1910 - acc: 0.7732 - val_loss: 1.2619 - val_acc: 0.7270\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1827 - acc: 0.7777 - val_loss: 1.2575 - val_acc: 0.7310\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1747 - acc: 0.7776 - val_loss: 1.2504 - val_acc: 0.7280\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1671 - acc: 0.7815 - val_loss: 1.2468 - val_acc: 0.7310\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1596 - acc: 0.7795 - val_loss: 1.2447 - val_acc: 0.7330\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1523 - acc: 0.7821 - val_loss: 1.2385 - val_acc: 0.7290\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1452 - acc: 0.7843 - val_loss: 1.2317 - val_acc: 0.7280\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1380 - acc: 0.7873 - val_loss: 1.2273 - val_acc: 0.7320\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1311 - acc: 0.7891 - val_loss: 1.2238 - val_acc: 0.7350\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1244 - acc: 0.7912 - val_loss: 1.2200 - val_acc: 0.7360\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1181 - acc: 0.7897 - val_loss: 1.2134 - val_acc: 0.7300\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1118 - acc: 0.7951 - val_loss: 1.2124 - val_acc: 0.7320\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1054 - acc: 0.7951 - val_loss: 1.2084 - val_acc: 0.7360\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0996 - acc: 0.7949 - val_loss: 1.2029 - val_acc: 0.7270\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0933 - acc: 0.7981 - val_loss: 1.1969 - val_acc: 0.7330\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0877 - acc: 0.7987 - val_loss: 1.1961 - val_acc: 0.7370\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0817 - acc: 0.8007 - val_loss: 1.1892 - val_acc: 0.7350\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0758 - acc: 0.8016 - val_loss: 1.1861 - val_acc: 0.7350\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0699 - acc: 0.8035 - val_loss: 1.1829 - val_acc: 0.7340\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0648 - acc: 0.8033 - val_loss: 1.1817 - val_acc: 0.7320\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0594 - acc: 0.8055 - val_loss: 1.1812 - val_acc: 0.7350\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0539 - acc: 0.8064 - val_loss: 1.1741 - val_acc: 0.7320\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0486 - acc: 0.8088 - val_loss: 1.1698 - val_acc: 0.7330\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0430 - acc: 0.8103 - val_loss: 1.1690 - val_acc: 0.7370\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0380 - acc: 0.8119 - val_loss: 1.1682 - val_acc: 0.7340\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0330 - acc: 0.8108 - val_loss: 1.1621 - val_acc: 0.7360\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0281 - acc: 0.8151 - val_loss: 1.1578 - val_acc: 0.7360\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0232 - acc: 0.8152 - val_loss: 1.1589 - val_acc: 0.7270\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0181 - acc: 0.8164 - val_loss: 1.1549 - val_acc: 0.7370\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0132 - acc: 0.8165 - val_loss: 1.1500 - val_acc: 0.7350\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0084 - acc: 0.8187 - val_loss: 1.1502 - val_acc: 0.7360\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0037 - acc: 0.8189 - val_loss: 1.1465 - val_acc: 0.7350\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9990 - acc: 0.8215 - val_loss: 1.1457 - val_acc: 0.7380\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9945 - acc: 0.8235 - val_loss: 1.1414 - val_acc: 0.7470\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9901 - acc: 0.8217 - val_loss: 1.1363 - val_acc: 0.7370\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9852 - acc: 0.8253 - val_loss: 1.1349 - val_acc: 0.7340\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9811 - acc: 0.8267 - val_loss: 1.1316 - val_acc: 0.7330\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9765 - acc: 0.8275 - val_loss: 1.1311 - val_acc: 0.7370\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9722 - acc: 0.8268 - val_loss: 1.1285 - val_acc: 0.7370\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9677 - acc: 0.8291 - val_loss: 1.1243 - val_acc: 0.7380\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9634 - acc: 0.8321 - val_loss: 1.1239 - val_acc: 0.7390\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9589 - acc: 0.8315 - val_loss: 1.1217 - val_acc: 0.7430\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9551 - acc: 0.8351 - val_loss: 1.1173 - val_acc: 0.7410\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9510 - acc: 0.8344 - val_loss: 1.1166 - val_acc: 0.7410\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9468 - acc: 0.8356 - val_loss: 1.1133 - val_acc: 0.7370\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9426 - acc: 0.8379 - val_loss: 1.1114 - val_acc: 0.7410\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9388 - acc: 0.8379 - val_loss: 1.1067 - val_acc: 0.7390\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9346 - acc: 0.8407 - val_loss: 1.1043 - val_acc: 0.7400\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9309 - acc: 0.8415 - val_loss: 1.1078 - val_acc: 0.7390\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9269 - acc: 0.8429 - val_loss: 1.1079 - val_acc: 0.7440\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9230 - acc: 0.8425 - val_loss: 1.1016 - val_acc: 0.7420\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9189 - acc: 0.8449 - val_loss: 1.0987 - val_acc: 0.7420\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9152 - acc: 0.8443 - val_loss: 1.0962 - val_acc: 0.7440\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9115 - acc: 0.8452 - val_loss: 1.1025 - val_acc: 0.7430\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9081 - acc: 0.8477 - val_loss: 1.0932 - val_acc: 0.7480\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9040 - acc: 0.8483 - val_loss: 1.0918 - val_acc: 0.7490\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8998 - acc: 0.8489 - val_loss: 1.0895 - val_acc: 0.7450\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8967 - acc: 0.8481 - val_loss: 1.0891 - val_acc: 0.7440\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8929 - acc: 0.8503 - val_loss: 1.0876 - val_acc: 0.7480\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8893 - acc: 0.8525 - val_loss: 1.0865 - val_acc: 0.7480\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8857 - acc: 0.8521 - val_loss: 1.0813 - val_acc: 0.7460\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8825 - acc: 0.8519 - val_loss: 1.0801 - val_acc: 0.7460\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8787 - acc: 0.8544 - val_loss: 1.0803 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8753 - acc: 0.8545 - val_loss: 1.0785 - val_acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 16.0462 - acc: 0.1173 - val_loss: 15.6268 - val_acc: 0.1210\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 15.2595 - acc: 0.1525 - val_loss: 14.8716 - val_acc: 0.1560\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 14.5226 - acc: 0.1999 - val_loss: 14.1477 - val_acc: 0.2140\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 13.8119 - acc: 0.2253 - val_loss: 13.4480 - val_acc: 0.2520\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 13.1241 - acc: 0.2485 - val_loss: 12.7710 - val_acc: 0.2820\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 12.4579 - acc: 0.2708 - val_loss: 12.1152 - val_acc: 0.3030\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 11.8120 - acc: 0.2891 - val_loss: 11.4798 - val_acc: 0.3200\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 11.1862 - acc: 0.3155 - val_loss: 10.8646 - val_acc: 0.3420\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 10.5800 - acc: 0.3393 - val_loss: 10.2692 - val_acc: 0.3740\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 9.9927 - acc: 0.3735 - val_loss: 9.6918 - val_acc: 0.4050\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 9.4241 - acc: 0.4095 - val_loss: 9.1332 - val_acc: 0.4220\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 8.8753 - acc: 0.4287 - val_loss: 8.5960 - val_acc: 0.4420\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 8.3465 - acc: 0.4467 - val_loss: 8.0775 - val_acc: 0.4570\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 7.8381 - acc: 0.4677 - val_loss: 7.5799 - val_acc: 0.4730\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 7.3510 - acc: 0.4793 - val_loss: 7.1041 - val_acc: 0.4900\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 6.8857 - acc: 0.4949 - val_loss: 6.6499 - val_acc: 0.4970\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 6.4419 - acc: 0.5075 - val_loss: 6.2170 - val_acc: 0.5100\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 6.0196 - acc: 0.5192 - val_loss: 5.8066 - val_acc: 0.5300\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 5.6194 - acc: 0.5371 - val_loss: 5.4169 - val_acc: 0.5370\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 5.2411 - acc: 0.5440 - val_loss: 5.0496 - val_acc: 0.5470\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 4.8850 - acc: 0.5543 - val_loss: 4.7045 - val_acc: 0.5480\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 4.5513 - acc: 0.5652 - val_loss: 4.3835 - val_acc: 0.5600\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 4.2402 - acc: 0.5712 - val_loss: 4.0824 - val_acc: 0.5700\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 3.9516 - acc: 0.5845 - val_loss: 3.8059 - val_acc: 0.5780\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 3.6854 - acc: 0.5913 - val_loss: 3.5496 - val_acc: 0.5750\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 3.4409 - acc: 0.5951 - val_loss: 3.3163 - val_acc: 0.5860\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 3.2177 - acc: 0.6025 - val_loss: 3.1048 - val_acc: 0.5980\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 3.0170 - acc: 0.6133 - val_loss: 2.9156 - val_acc: 0.5990\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.8373 - acc: 0.6164 - val_loss: 2.7451 - val_acc: 0.6090\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.6787 - acc: 0.6221 - val_loss: 2.5972 - val_acc: 0.6170\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.5407 - acc: 0.6287 - val_loss: 2.4701 - val_acc: 0.6150\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.4229 - acc: 0.6304 - val_loss: 2.3632 - val_acc: 0.6250\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.3252 - acc: 0.6316 - val_loss: 2.2754 - val_acc: 0.6290\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.2462 - acc: 0.6372 - val_loss: 2.2043 - val_acc: 0.6180\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.1848 - acc: 0.6368 - val_loss: 2.1521 - val_acc: 0.6360\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.1382 - acc: 0.6407 - val_loss: 2.1112 - val_acc: 0.6380\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.1040 - acc: 0.6452 - val_loss: 2.0797 - val_acc: 0.6400\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.0757 - acc: 0.6485 - val_loss: 2.0547 - val_acc: 0.6520\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.0504 - acc: 0.6500 - val_loss: 2.0337 - val_acc: 0.6390\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.0272 - acc: 0.6552 - val_loss: 2.0060 - val_acc: 0.6540\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.0049 - acc: 0.6560 - val_loss: 1.9858 - val_acc: 0.6590\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9844 - acc: 0.6583 - val_loss: 1.9649 - val_acc: 0.6590\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9641 - acc: 0.6599 - val_loss: 1.9451 - val_acc: 0.6620\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9451 - acc: 0.6620 - val_loss: 1.9269 - val_acc: 0.6610\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9268 - acc: 0.6651 - val_loss: 1.9099 - val_acc: 0.6600\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9085 - acc: 0.6641 - val_loss: 1.8935 - val_acc: 0.6590\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8913 - acc: 0.6660 - val_loss: 1.8760 - val_acc: 0.6570\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8747 - acc: 0.6669 - val_loss: 1.8596 - val_acc: 0.6670\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8587 - acc: 0.6696 - val_loss: 1.8452 - val_acc: 0.6680\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8433 - acc: 0.6705 - val_loss: 1.8317 - val_acc: 0.6650\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8280 - acc: 0.6713 - val_loss: 1.8153 - val_acc: 0.6740\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8136 - acc: 0.6729 - val_loss: 1.8008 - val_acc: 0.6710\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7990 - acc: 0.6751 - val_loss: 1.7890 - val_acc: 0.6740\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7856 - acc: 0.6743 - val_loss: 1.7742 - val_acc: 0.6690\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7719 - acc: 0.6785 - val_loss: 1.7667 - val_acc: 0.6700\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7586 - acc: 0.6785 - val_loss: 1.7502 - val_acc: 0.6750\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7463 - acc: 0.6781 - val_loss: 1.7401 - val_acc: 0.6730\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7339 - acc: 0.6796 - val_loss: 1.7294 - val_acc: 0.6710\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7224 - acc: 0.6789 - val_loss: 1.7177 - val_acc: 0.6740\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7109 - acc: 0.6821 - val_loss: 1.7043 - val_acc: 0.6740\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6994 - acc: 0.6843 - val_loss: 1.6950 - val_acc: 0.6750\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6891 - acc: 0.6836 - val_loss: 1.6854 - val_acc: 0.6760\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6777 - acc: 0.6860 - val_loss: 1.6742 - val_acc: 0.6770\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6678 - acc: 0.6832 - val_loss: 1.6640 - val_acc: 0.6740\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6570 - acc: 0.6865 - val_loss: 1.6550 - val_acc: 0.6760\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6469 - acc: 0.6871 - val_loss: 1.6465 - val_acc: 0.6800\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6371 - acc: 0.6875 - val_loss: 1.6358 - val_acc: 0.6840\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6275 - acc: 0.6893 - val_loss: 1.6285 - val_acc: 0.6760\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6183 - acc: 0.6900 - val_loss: 1.6180 - val_acc: 0.6760\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6092 - acc: 0.6913 - val_loss: 1.6093 - val_acc: 0.6810\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5995 - acc: 0.6925 - val_loss: 1.6026 - val_acc: 0.6800\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5907 - acc: 0.6900 - val_loss: 1.5942 - val_acc: 0.6850\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5820 - acc: 0.6939 - val_loss: 1.5838 - val_acc: 0.6830\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5737 - acc: 0.6941 - val_loss: 1.5780 - val_acc: 0.6840\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5656 - acc: 0.6952 - val_loss: 1.5692 - val_acc: 0.6860\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5566 - acc: 0.6947 - val_loss: 1.5624 - val_acc: 0.6830\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5486 - acc: 0.6969 - val_loss: 1.5539 - val_acc: 0.6810\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5409 - acc: 0.6960 - val_loss: 1.5478 - val_acc: 0.6900\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5333 - acc: 0.6972 - val_loss: 1.5386 - val_acc: 0.6850\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5252 - acc: 0.6955 - val_loss: 1.5331 - val_acc: 0.6890\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5171 - acc: 0.6981 - val_loss: 1.5286 - val_acc: 0.6880\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.5102 - acc: 0.6980 - val_loss: 1.5157 - val_acc: 0.6860\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5026 - acc: 0.6975 - val_loss: 1.5119 - val_acc: 0.6910\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4950 - acc: 0.7005 - val_loss: 1.5045 - val_acc: 0.6930\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4877 - acc: 0.7011 - val_loss: 1.4961 - val_acc: 0.6890\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4804 - acc: 0.7021 - val_loss: 1.4891 - val_acc: 0.6870\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4740 - acc: 0.7011 - val_loss: 1.4832 - val_acc: 0.6930\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4666 - acc: 0.7019 - val_loss: 1.4761 - val_acc: 0.6900\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4602 - acc: 0.7020 - val_loss: 1.4695 - val_acc: 0.6920\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4530 - acc: 0.7057 - val_loss: 1.4633 - val_acc: 0.6940\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4460 - acc: 0.7040 - val_loss: 1.4622 - val_acc: 0.6820\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4397 - acc: 0.7068 - val_loss: 1.4549 - val_acc: 0.6940\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4334 - acc: 0.7044 - val_loss: 1.4462 - val_acc: 0.6930\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4270 - acc: 0.7049 - val_loss: 1.4401 - val_acc: 0.6960\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4205 - acc: 0.7057 - val_loss: 1.4340 - val_acc: 0.6910\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4145 - acc: 0.7076 - val_loss: 1.4322 - val_acc: 0.6900\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4084 - acc: 0.7071 - val_loss: 1.4215 - val_acc: 0.6970\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4018 - acc: 0.7107 - val_loss: 1.4176 - val_acc: 0.6950\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3963 - acc: 0.7097 - val_loss: 1.4109 - val_acc: 0.6960\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3900 - acc: 0.7105 - val_loss: 1.4078 - val_acc: 0.7000\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3843 - acc: 0.7099 - val_loss: 1.4015 - val_acc: 0.7000\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3785 - acc: 0.7105 - val_loss: 1.3956 - val_acc: 0.7000\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3727 - acc: 0.7117 - val_loss: 1.3908 - val_acc: 0.6970\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3669 - acc: 0.7115 - val_loss: 1.3831 - val_acc: 0.6960\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3614 - acc: 0.7116 - val_loss: 1.3823 - val_acc: 0.6960\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3555 - acc: 0.7128 - val_loss: 1.3725 - val_acc: 0.6960\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3498 - acc: 0.7140 - val_loss: 1.3699 - val_acc: 0.6980\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3447 - acc: 0.7113 - val_loss: 1.3683 - val_acc: 0.6990\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3401 - acc: 0.7129 - val_loss: 1.3611 - val_acc: 0.6960\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3341 - acc: 0.7121 - val_loss: 1.3534 - val_acc: 0.7010\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3288 - acc: 0.7140 - val_loss: 1.3496 - val_acc: 0.6980\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3241 - acc: 0.7152 - val_loss: 1.3512 - val_acc: 0.7050\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3186 - acc: 0.7177 - val_loss: 1.3394 - val_acc: 0.7020\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3135 - acc: 0.7163 - val_loss: 1.3399 - val_acc: 0.7070\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3090 - acc: 0.7164 - val_loss: 1.3303 - val_acc: 0.7020\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3034 - acc: 0.7175 - val_loss: 1.3282 - val_acc: 0.6990\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2991 - acc: 0.7169 - val_loss: 1.3230 - val_acc: 0.7050\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2934 - acc: 0.7204 - val_loss: 1.3170 - val_acc: 0.7060\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2895 - acc: 0.7192 - val_loss: 1.3116 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2842 - acc: 0.7187 - val_loss: 1.3104 - val_acc: 0.7000\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 16.0219 - acc: 0.1732 - val_loss: 15.6058 - val_acc: 0.2250\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 15.2575 - acc: 0.1993 - val_loss: 14.8561 - val_acc: 0.2370\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 14.5197 - acc: 0.2095 - val_loss: 14.1300 - val_acc: 0.2470\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 13.8045 - acc: 0.2173 - val_loss: 13.4256 - val_acc: 0.2520\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 13.1104 - acc: 0.2309 - val_loss: 12.7416 - val_acc: 0.2730\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 12.4368 - acc: 0.2523 - val_loss: 12.0777 - val_acc: 0.2920\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 11.7833 - acc: 0.2761 - val_loss: 11.4333 - val_acc: 0.3320\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 11.1493 - acc: 0.3088 - val_loss: 10.8094 - val_acc: 0.3540\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 10.5354 - acc: 0.3373 - val_loss: 10.2056 - val_acc: 0.3750\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 9.9418 - acc: 0.3608 - val_loss: 9.6226 - val_acc: 0.3970\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 9.3686 - acc: 0.3933 - val_loss: 9.0597 - val_acc: 0.4240\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 8.8161 - acc: 0.4220 - val_loss: 8.5184 - val_acc: 0.4380\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 8.2846 - acc: 0.4501 - val_loss: 7.9978 - val_acc: 0.4780\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 7.7743 - acc: 0.4839 - val_loss: 7.4994 - val_acc: 0.4950\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 7.2864 - acc: 0.5075 - val_loss: 7.0241 - val_acc: 0.5150\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 6.8218 - acc: 0.5283 - val_loss: 6.5713 - val_acc: 0.5320\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 6.3798 - acc: 0.5477 - val_loss: 6.1406 - val_acc: 0.5450\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 5.9597 - acc: 0.5615 - val_loss: 5.7331 - val_acc: 0.5540\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 5.5620 - acc: 0.5773 - val_loss: 5.3480 - val_acc: 0.5710\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 5.1867 - acc: 0.5933 - val_loss: 4.9845 - val_acc: 0.5760\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 4.8338 - acc: 0.6037 - val_loss: 4.6444 - val_acc: 0.5880\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 4.5035 - acc: 0.6140 - val_loss: 4.3271 - val_acc: 0.6070\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 4.1962 - acc: 0.6240 - val_loss: 4.0330 - val_acc: 0.6230\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 3.9115 - acc: 0.6337 - val_loss: 3.7588 - val_acc: 0.6270\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 3.6491 - acc: 0.6424 - val_loss: 3.5094 - val_acc: 0.6200\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 3.4092 - acc: 0.6440 - val_loss: 3.2806 - val_acc: 0.6390\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 3.1910 - acc: 0.6501 - val_loss: 3.0740 - val_acc: 0.6530\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.9945 - acc: 0.6560 - val_loss: 2.8884 - val_acc: 0.6530\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.8185 - acc: 0.6585 - val_loss: 2.7232 - val_acc: 0.6600\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.6631 - acc: 0.6605 - val_loss: 2.5792 - val_acc: 0.6610\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.5287 - acc: 0.6644 - val_loss: 2.4556 - val_acc: 0.6580\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.4142 - acc: 0.6665 - val_loss: 2.3508 - val_acc: 0.6600\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.3190 - acc: 0.6669 - val_loss: 2.2657 - val_acc: 0.6690\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2424 - acc: 0.6703 - val_loss: 2.1987 - val_acc: 0.6720\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.1830 - acc: 0.6696 - val_loss: 2.1475 - val_acc: 0.6700\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.1386 - acc: 0.6732 - val_loss: 2.1090 - val_acc: 0.6730\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.1049 - acc: 0.6724 - val_loss: 2.0809 - val_acc: 0.6730\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.0772 - acc: 0.6753 - val_loss: 2.0557 - val_acc: 0.6740\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.0529 - acc: 0.6801 - val_loss: 2.0322 - val_acc: 0.6780\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0305 - acc: 0.6793 - val_loss: 2.0107 - val_acc: 0.6750\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0102 - acc: 0.6819 - val_loss: 1.9905 - val_acc: 0.6790\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9904 - acc: 0.6832 - val_loss: 1.9726 - val_acc: 0.6910\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9718 - acc: 0.6844 - val_loss: 1.9556 - val_acc: 0.6880\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9546 - acc: 0.6864 - val_loss: 1.9365 - val_acc: 0.6830\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9371 - acc: 0.6845 - val_loss: 1.9228 - val_acc: 0.6870\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.9208 - acc: 0.6871 - val_loss: 1.9059 - val_acc: 0.6790\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9050 - acc: 0.6888 - val_loss: 1.8920 - val_acc: 0.6860\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8898 - acc: 0.6896 - val_loss: 1.8766 - val_acc: 0.6860\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8749 - acc: 0.6899 - val_loss: 1.8646 - val_acc: 0.6760\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8605 - acc: 0.6911 - val_loss: 1.8483 - val_acc: 0.6880\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8468 - acc: 0.6925 - val_loss: 1.8346 - val_acc: 0.6810\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8333 - acc: 0.6943 - val_loss: 1.8221 - val_acc: 0.6830\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8199 - acc: 0.6933 - val_loss: 1.8085 - val_acc: 0.6890\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8071 - acc: 0.6937 - val_loss: 1.7986 - val_acc: 0.6810\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7947 - acc: 0.6961 - val_loss: 1.7871 - val_acc: 0.6820\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7825 - acc: 0.6959 - val_loss: 1.7741 - val_acc: 0.6910\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7703 - acc: 0.6985 - val_loss: 1.7626 - val_acc: 0.6880\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7589 - acc: 0.6973 - val_loss: 1.7520 - val_acc: 0.6890\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7474 - acc: 0.6985 - val_loss: 1.7388 - val_acc: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7360 - acc: 0.6991 - val_loss: 1.7307 - val_acc: 0.6920\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7253 - acc: 0.7020 - val_loss: 1.7213 - val_acc: 0.6920\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7148 - acc: 0.7012 - val_loss: 1.7082 - val_acc: 0.6920\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7046 - acc: 0.7005 - val_loss: 1.6985 - val_acc: 0.6920\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6936 - acc: 0.7035 - val_loss: 1.6870 - val_acc: 0.6940\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6838 - acc: 0.7044 - val_loss: 1.6779 - val_acc: 0.6980\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6744 - acc: 0.7053 - val_loss: 1.6688 - val_acc: 0.6990\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6649 - acc: 0.7049 - val_loss: 1.6611 - val_acc: 0.6950\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6555 - acc: 0.7059 - val_loss: 1.6531 - val_acc: 0.6940\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6466 - acc: 0.7075 - val_loss: 1.6478 - val_acc: 0.6900\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6376 - acc: 0.7072 - val_loss: 1.6354 - val_acc: 0.6980\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6285 - acc: 0.7097 - val_loss: 1.6258 - val_acc: 0.6980\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6195 - acc: 0.7100 - val_loss: 1.6181 - val_acc: 0.7010\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6116 - acc: 0.7105 - val_loss: 1.6095 - val_acc: 0.6990\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6031 - acc: 0.7105 - val_loss: 1.6029 - val_acc: 0.6950\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5951 - acc: 0.7119 - val_loss: 1.5940 - val_acc: 0.6990\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5868 - acc: 0.7119 - val_loss: 1.5857 - val_acc: 0.7010\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5787 - acc: 0.7135 - val_loss: 1.5769 - val_acc: 0.7050\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5712 - acc: 0.7148 - val_loss: 1.5757 - val_acc: 0.7010\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5636 - acc: 0.7149 - val_loss: 1.5661 - val_acc: 0.7000\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5558 - acc: 0.7168 - val_loss: 1.5558 - val_acc: 0.7020\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5484 - acc: 0.7175 - val_loss: 1.5509 - val_acc: 0.7060\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5414 - acc: 0.7173 - val_loss: 1.5441 - val_acc: 0.7040\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5340 - acc: 0.7181 - val_loss: 1.5358 - val_acc: 0.7060\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5269 - acc: 0.7191 - val_loss: 1.5280 - val_acc: 0.7030\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5201 - acc: 0.7188 - val_loss: 1.5231 - val_acc: 0.7070\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5134 - acc: 0.7191 - val_loss: 1.5185 - val_acc: 0.7050\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5062 - acc: 0.7213 - val_loss: 1.5099 - val_acc: 0.7010\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4997 - acc: 0.7195 - val_loss: 1.5047 - val_acc: 0.7080\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4931 - acc: 0.7201 - val_loss: 1.4956 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4865 - acc: 0.7211 - val_loss: 1.4910 - val_acc: 0.7030\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4806 - acc: 0.7221 - val_loss: 1.4854 - val_acc: 0.7070\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4738 - acc: 0.7223 - val_loss: 1.4853 - val_acc: 0.7050\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4679 - acc: 0.7236 - val_loss: 1.4710 - val_acc: 0.7120\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4610 - acc: 0.7239 - val_loss: 1.4649 - val_acc: 0.7090\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4551 - acc: 0.7239 - val_loss: 1.4610 - val_acc: 0.7130\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4496 - acc: 0.7240 - val_loss: 1.4586 - val_acc: 0.7020\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4433 - acc: 0.7245 - val_loss: 1.4517 - val_acc: 0.7070\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4371 - acc: 0.7252 - val_loss: 1.4444 - val_acc: 0.7070\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4316 - acc: 0.7253 - val_loss: 1.4360 - val_acc: 0.7110\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4257 - acc: 0.7271 - val_loss: 1.4331 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4203 - acc: 0.7264 - val_loss: 1.4297 - val_acc: 0.7110\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4149 - acc: 0.7287 - val_loss: 1.4208 - val_acc: 0.7140\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4092 - acc: 0.7268 - val_loss: 1.4169 - val_acc: 0.7130\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4035 - acc: 0.7285 - val_loss: 1.4101 - val_acc: 0.7180\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3977 - acc: 0.7275 - val_loss: 1.4067 - val_acc: 0.7140\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3929 - acc: 0.7289 - val_loss: 1.4056 - val_acc: 0.7120\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3876 - acc: 0.7291 - val_loss: 1.3951 - val_acc: 0.7190\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3822 - acc: 0.7296 - val_loss: 1.3924 - val_acc: 0.7120\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.3772 - acc: 0.7305 - val_loss: 1.3865 - val_acc: 0.7150\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3720 - acc: 0.7304 - val_loss: 1.3811 - val_acc: 0.7170\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3667 - acc: 0.7305 - val_loss: 1.3780 - val_acc: 0.7170\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3620 - acc: 0.7311 - val_loss: 1.3729 - val_acc: 0.7180\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3569 - acc: 0.7313 - val_loss: 1.3679 - val_acc: 0.7110\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3522 - acc: 0.7344 - val_loss: 1.3613 - val_acc: 0.7160\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3474 - acc: 0.7319 - val_loss: 1.3594 - val_acc: 0.7160\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3422 - acc: 0.7325 - val_loss: 1.3568 - val_acc: 0.7120\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3380 - acc: 0.7341 - val_loss: 1.3485 - val_acc: 0.7150\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3328 - acc: 0.7337 - val_loss: 1.3439 - val_acc: 0.7190\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3284 - acc: 0.7323 - val_loss: 1.3396 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3235 - acc: 0.7353 - val_loss: 1.3357 - val_acc: 0.7220\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3191 - acc: 0.7357 - val_loss: 1.3345 - val_acc: 0.7130\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3148 - acc: 0.7357 - val_loss: 1.3287 - val_acc: 0.7130\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3103 - acc: 0.7345 - val_loss: 1.3227 - val_acc: 0.7180\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3057 - acc: 0.7371 - val_loss: 1.3198 - val_acc: 0.7170\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3020 - acc: 0.7364 - val_loss: 1.3167 - val_acc: 0.7100\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2978 - acc: 0.7359 - val_loss: 1.3130 - val_acc: 0.7190\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2932 - acc: 0.7355 - val_loss: 1.3071 - val_acc: 0.7190\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2885 - acc: 0.7380 - val_loss: 1.3036 - val_acc: 0.7190\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2843 - acc: 0.7367 - val_loss: 1.3001 - val_acc: 0.7170\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2807 - acc: 0.7379 - val_loss: 1.3004 - val_acc: 0.7210\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2769 - acc: 0.7384 - val_loss: 1.2979 - val_acc: 0.7140\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2732 - acc: 0.7369 - val_loss: 1.2856 - val_acc: 0.7230\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2684 - acc: 0.7381 - val_loss: 1.2880 - val_acc: 0.7230\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2644 - acc: 0.7393 - val_loss: 1.2827 - val_acc: 0.7160\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2613 - acc: 0.7393 - val_loss: 1.2759 - val_acc: 0.7300\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2571 - acc: 0.7407 - val_loss: 1.2762 - val_acc: 0.7200\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2532 - acc: 0.7404 - val_loss: 1.2697 - val_acc: 0.7250\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2489 - acc: 0.7389 - val_loss: 1.2658 - val_acc: 0.7170\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2461 - acc: 0.7407 - val_loss: 1.2624 - val_acc: 0.7340\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.2422 - acc: 0.7403 - val_loss: 1.2601 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2386 - acc: 0.7377 - val_loss: 1.2615 - val_acc: 0.7320\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2354 - acc: 0.7401 - val_loss: 1.2522 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2317 - acc: 0.7407 - val_loss: 1.2499 - val_acc: 0.7280\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2277 - acc: 0.7419 - val_loss: 1.2463 - val_acc: 0.7230\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2248 - acc: 0.7415 - val_loss: 1.2436 - val_acc: 0.7250\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2212 - acc: 0.7408 - val_loss: 1.2407 - val_acc: 0.7350\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2184 - acc: 0.7416 - val_loss: 1.2360 - val_acc: 0.7280\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2150 - acc: 0.7436 - val_loss: 1.2358 - val_acc: 0.7250\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2114 - acc: 0.7405 - val_loss: 1.2346 - val_acc: 0.7210\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2081 - acc: 0.7439 - val_loss: 1.2304 - val_acc: 0.7230\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2050 - acc: 0.7443 - val_loss: 1.2264 - val_acc: 0.7290\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2017 - acc: 0.7440 - val_loss: 1.2265 - val_acc: 0.7190\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1990 - acc: 0.7431 - val_loss: 1.2203 - val_acc: 0.7240\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1956 - acc: 0.7445 - val_loss: 1.2162 - val_acc: 0.7310\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1930 - acc: 0.7428 - val_loss: 1.2158 - val_acc: 0.7360\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1905 - acc: 0.7423 - val_loss: 1.2124 - val_acc: 0.7260\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1877 - acc: 0.7443 - val_loss: 1.2104 - val_acc: 0.7390\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1851 - acc: 0.7429 - val_loss: 1.2045 - val_acc: 0.7250\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1817 - acc: 0.7443 - val_loss: 1.2039 - val_acc: 0.7310\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1787 - acc: 0.7445 - val_loss: 1.2002 - val_acc: 0.7280\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1762 - acc: 0.7431 - val_loss: 1.1993 - val_acc: 0.7250\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1735 - acc: 0.7443 - val_loss: 1.1969 - val_acc: 0.7330\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1712 - acc: 0.7451 - val_loss: 1.1934 - val_acc: 0.7340\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1690 - acc: 0.7461 - val_loss: 1.1940 - val_acc: 0.7410\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1663 - acc: 0.7443 - val_loss: 1.1922 - val_acc: 0.7300\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1643 - acc: 0.7453 - val_loss: 1.1906 - val_acc: 0.7230\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1615 - acc: 0.7465 - val_loss: 1.1862 - val_acc: 0.7260\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1587 - acc: 0.7468 - val_loss: 1.1856 - val_acc: 0.7230\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1568 - acc: 0.7461 - val_loss: 1.1787 - val_acc: 0.7260\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1544 - acc: 0.7445 - val_loss: 1.1798 - val_acc: 0.7220\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1518 - acc: 0.7475 - val_loss: 1.1772 - val_acc: 0.7340\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1504 - acc: 0.7471 - val_loss: 1.1817 - val_acc: 0.7210\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1477 - acc: 0.7476 - val_loss: 1.1703 - val_acc: 0.7360\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1458 - acc: 0.7465 - val_loss: 1.1706 - val_acc: 0.7370\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1436 - acc: 0.7467 - val_loss: 1.1704 - val_acc: 0.7330\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1414 - acc: 0.7464 - val_loss: 1.1656 - val_acc: 0.7300\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1397 - acc: 0.7476 - val_loss: 1.1644 - val_acc: 0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1379 - acc: 0.7479 - val_loss: 1.1659 - val_acc: 0.7260\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1357 - acc: 0.7481 - val_loss: 1.1610 - val_acc: 0.7350\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1331 - acc: 0.7488 - val_loss: 1.1631 - val_acc: 0.7350\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1322 - acc: 0.7487 - val_loss: 1.1608 - val_acc: 0.7280\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1301 - acc: 0.7471 - val_loss: 1.1560 - val_acc: 0.7320\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1281 - acc: 0.7473 - val_loss: 1.1584 - val_acc: 0.7320\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1259 - acc: 0.7484 - val_loss: 1.1556 - val_acc: 0.7290\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1241 - acc: 0.7493 - val_loss: 1.1511 - val_acc: 0.7350\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1225 - acc: 0.7481 - val_loss: 1.1529 - val_acc: 0.7300\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1205 - acc: 0.7491 - val_loss: 1.1490 - val_acc: 0.7300\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1191 - acc: 0.7471 - val_loss: 1.1452 - val_acc: 0.7360\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1173 - acc: 0.7491 - val_loss: 1.1508 - val_acc: 0.7340\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1159 - acc: 0.7483 - val_loss: 1.1414 - val_acc: 0.7350\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1140 - acc: 0.7501 - val_loss: 1.1416 - val_acc: 0.7310\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1126 - acc: 0.7505 - val_loss: 1.1410 - val_acc: 0.7270\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1111 - acc: 0.7488 - val_loss: 1.1430 - val_acc: 0.7330\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1098 - acc: 0.7508 - val_loss: 1.1431 - val_acc: 0.7310\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1087 - acc: 0.7497 - val_loss: 1.1339 - val_acc: 0.7430\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1067 - acc: 0.7513 - val_loss: 1.1383 - val_acc: 0.7320\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1056 - acc: 0.7504 - val_loss: 1.1343 - val_acc: 0.7320\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1037 - acc: 0.7500 - val_loss: 1.1350 - val_acc: 0.7320\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1021 - acc: 0.7499 - val_loss: 1.1327 - val_acc: 0.7370\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1008 - acc: 0.7511 - val_loss: 1.1390 - val_acc: 0.7270\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0994 - acc: 0.7516 - val_loss: 1.1276 - val_acc: 0.7450\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0977 - acc: 0.7487 - val_loss: 1.1309 - val_acc: 0.7460\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0968 - acc: 0.7504 - val_loss: 1.1254 - val_acc: 0.7500\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0952 - acc: 0.7531 - val_loss: 1.1295 - val_acc: 0.7340\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0935 - acc: 0.7527 - val_loss: 1.1277 - val_acc: 0.7380\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0928 - acc: 0.7503 - val_loss: 1.1231 - val_acc: 0.7500\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0916 - acc: 0.7511 - val_loss: 1.1192 - val_acc: 0.7360\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0902 - acc: 0.7513 - val_loss: 1.1240 - val_acc: 0.7340\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0889 - acc: 0.7517 - val_loss: 1.1171 - val_acc: 0.7470\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0877 - acc: 0.7513 - val_loss: 1.1187 - val_acc: 0.7390\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0862 - acc: 0.7525 - val_loss: 1.1163 - val_acc: 0.7440\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0857 - acc: 0.7531 - val_loss: 1.1178 - val_acc: 0.7380\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0839 - acc: 0.7532 - val_loss: 1.1146 - val_acc: 0.7380\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0828 - acc: 0.7511 - val_loss: 1.1117 - val_acc: 0.7420\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0810 - acc: 0.7527 - val_loss: 1.1158 - val_acc: 0.7380\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0807 - acc: 0.7520 - val_loss: 1.1113 - val_acc: 0.7400\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0796 - acc: 0.7524 - val_loss: 1.1088 - val_acc: 0.7400\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0778 - acc: 0.7525 - val_loss: 1.1103 - val_acc: 0.7390\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0766 - acc: 0.7528 - val_loss: 1.1063 - val_acc: 0.7510\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0753 - acc: 0.7525 - val_loss: 1.1037 - val_acc: 0.7470\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0740 - acc: 0.7524 - val_loss: 1.1077 - val_acc: 0.7470\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0730 - acc: 0.7541 - val_loss: 1.1060 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0718 - acc: 0.7532 - val_loss: 1.1020 - val_acc: 0.7490\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0709 - acc: 0.7529 - val_loss: 1.1070 - val_acc: 0.7400\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0699 - acc: 0.7541 - val_loss: 1.0985 - val_acc: 0.7530\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0687 - acc: 0.7539 - val_loss: 1.0981 - val_acc: 0.7410\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0672 - acc: 0.7536 - val_loss: 1.0982 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0668 - acc: 0.7552 - val_loss: 1.0978 - val_acc: 0.7390\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0653 - acc: 0.7547 - val_loss: 1.0961 - val_acc: 0.7470\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0642 - acc: 0.7567 - val_loss: 1.1030 - val_acc: 0.7430\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0635 - acc: 0.7537 - val_loss: 1.0999 - val_acc: 0.7330\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0624 - acc: 0.7553 - val_loss: 1.0941 - val_acc: 0.7430\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0611 - acc: 0.7529 - val_loss: 1.0891 - val_acc: 0.7510\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0603 - acc: 0.7532 - val_loss: 1.0914 - val_acc: 0.7530\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0590 - acc: 0.7552 - val_loss: 1.0907 - val_acc: 0.7520\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0578 - acc: 0.7543 - val_loss: 1.1040 - val_acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0572 - acc: 0.7537 - val_loss: 1.0887 - val_acc: 0.7480\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0562 - acc: 0.7547 - val_loss: 1.0949 - val_acc: 0.7430\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.0554 - acc: 0.7543 - val_loss: 1.0891 - val_acc: 0.7480\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0546 - acc: 0.7551 - val_loss: 1.0878 - val_acc: 0.7490\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0530 - acc: 0.7557 - val_loss: 1.0850 - val_acc: 0.7480\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0519 - acc: 0.7567 - val_loss: 1.0884 - val_acc: 0.7440\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0514 - acc: 0.7576 - val_loss: 1.0851 - val_acc: 0.7490\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0504 - acc: 0.7544 - val_loss: 1.0807 - val_acc: 0.7490\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0496 - acc: 0.7547 - val_loss: 1.0820 - val_acc: 0.7510\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0486 - acc: 0.7564 - val_loss: 1.0909 - val_acc: 0.7410\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0480 - acc: 0.7565 - val_loss: 1.0825 - val_acc: 0.7520\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0469 - acc: 0.7572 - val_loss: 1.0779 - val_acc: 0.7540\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0451 - acc: 0.7569 - val_loss: 1.0778 - val_acc: 0.7530\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0458 - acc: 0.7561 - val_loss: 1.0804 - val_acc: 0.7490\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0437 - acc: 0.7564 - val_loss: 1.0768 - val_acc: 0.7570\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0435 - acc: 0.7575 - val_loss: 1.0773 - val_acc: 0.7500\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0424 - acc: 0.7563 - val_loss: 1.0739 - val_acc: 0.7510\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0411 - acc: 0.7573 - val_loss: 1.0776 - val_acc: 0.7420\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0406 - acc: 0.7567 - val_loss: 1.0787 - val_acc: 0.7500\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0397 - acc: 0.7560 - val_loss: 1.0779 - val_acc: 0.7440\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0384 - acc: 0.7583 - val_loss: 1.0772 - val_acc: 0.7500\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0379 - acc: 0.7564 - val_loss: 1.0759 - val_acc: 0.7490\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0371 - acc: 0.7571 - val_loss: 1.0750 - val_acc: 0.7500\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0370 - acc: 0.7591 - val_loss: 1.0685 - val_acc: 0.7570\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0355 - acc: 0.7568 - val_loss: 1.0762 - val_acc: 0.7470\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0348 - acc: 0.7579 - val_loss: 1.0698 - val_acc: 0.7530\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0335 - acc: 0.7587 - val_loss: 1.0655 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0334 - acc: 0.7565 - val_loss: 1.0651 - val_acc: 0.7510\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0317 - acc: 0.7585 - val_loss: 1.0714 - val_acc: 0.7460\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0312 - acc: 0.7559 - val_loss: 1.0667 - val_acc: 0.7540\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0300 - acc: 0.7581 - val_loss: 1.0625 - val_acc: 0.7510\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0300 - acc: 0.7583 - val_loss: 1.0654 - val_acc: 0.7500\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0294 - acc: 0.7596 - val_loss: 1.0682 - val_acc: 0.7440\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0285 - acc: 0.7587 - val_loss: 1.0618 - val_acc: 0.7530\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0274 - acc: 0.7584 - val_loss: 1.0672 - val_acc: 0.7470\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0267 - acc: 0.7589 - val_loss: 1.0636 - val_acc: 0.7590\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0264 - acc: 0.7575 - val_loss: 1.0610 - val_acc: 0.7530\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0255 - acc: 0.7596 - val_loss: 1.0611 - val_acc: 0.7470\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0244 - acc: 0.7596 - val_loss: 1.0583 - val_acc: 0.7590\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0232 - acc: 0.7596 - val_loss: 1.0576 - val_acc: 0.7530\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0225 - acc: 0.7592 - val_loss: 1.0553 - val_acc: 0.7530\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0220 - acc: 0.7597 - val_loss: 1.0566 - val_acc: 0.7540\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0211 - acc: 0.7601 - val_loss: 1.0552 - val_acc: 0.7510\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0208 - acc: 0.7595 - val_loss: 1.0572 - val_acc: 0.7560\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0196 - acc: 0.7595 - val_loss: 1.0550 - val_acc: 0.7550\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0190 - acc: 0.7603 - val_loss: 1.0557 - val_acc: 0.7600\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0185 - acc: 0.7576 - val_loss: 1.0562 - val_acc: 0.7540\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0178 - acc: 0.7616 - val_loss: 1.0539 - val_acc: 0.7530\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0168 - acc: 0.7607 - val_loss: 1.0579 - val_acc: 0.7510\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0160 - acc: 0.7601 - val_loss: 1.0580 - val_acc: 0.7490\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0158 - acc: 0.7604 - val_loss: 1.0520 - val_acc: 0.7580\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0143 - acc: 0.7603 - val_loss: 1.0555 - val_acc: 0.7540\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0143 - acc: 0.7615 - val_loss: 1.0509 - val_acc: 0.7520\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0135 - acc: 0.7585 - val_loss: 1.0471 - val_acc: 0.7560\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0127 - acc: 0.7599 - val_loss: 1.0579 - val_acc: 0.7470\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0121 - acc: 0.7613 - val_loss: 1.0540 - val_acc: 0.7560\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0110 - acc: 0.7620 - val_loss: 1.0550 - val_acc: 0.7460\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0109 - acc: 0.7595 - val_loss: 1.0482 - val_acc: 0.7510\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0093 - acc: 0.7628 - val_loss: 1.0515 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0095 - acc: 0.7619 - val_loss: 1.0498 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0084 - acc: 0.7609 - val_loss: 1.0487 - val_acc: 0.7550\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0076 - acc: 0.7616 - val_loss: 1.0447 - val_acc: 0.7570\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0069 - acc: 0.7611 - val_loss: 1.0477 - val_acc: 0.7560\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0070 - acc: 0.7621 - val_loss: 1.0497 - val_acc: 0.7510\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0061 - acc: 0.7603 - val_loss: 1.0492 - val_acc: 0.7570\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0058 - acc: 0.7604 - val_loss: 1.0446 - val_acc: 0.7590\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0043 - acc: 0.7620 - val_loss: 1.0496 - val_acc: 0.7440\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0036 - acc: 0.7629 - val_loss: 1.0424 - val_acc: 0.7560\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0033 - acc: 0.7613 - val_loss: 1.0441 - val_acc: 0.7500\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0026 - acc: 0.7619 - val_loss: 1.0387 - val_acc: 0.7590\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0017 - acc: 0.7633 - val_loss: 1.0455 - val_acc: 0.7510\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0021 - acc: 0.7631 - val_loss: 1.0388 - val_acc: 0.7600\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0000 - acc: 0.7635 - val_loss: 1.0384 - val_acc: 0.7590\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9997 - acc: 0.7623 - val_loss: 1.0359 - val_acc: 0.7600\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9991 - acc: 0.7623 - val_loss: 1.0380 - val_acc: 0.7560\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9983 - acc: 0.7608 - val_loss: 1.0566 - val_acc: 0.7470\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9983 - acc: 0.7625 - val_loss: 1.0380 - val_acc: 0.7540\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9973 - acc: 0.7628 - val_loss: 1.0362 - val_acc: 0.7590\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9968 - acc: 0.7624 - val_loss: 1.0350 - val_acc: 0.7590\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9963 - acc: 0.7621 - val_loss: 1.0368 - val_acc: 0.7580\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9956 - acc: 0.7653 - val_loss: 1.0367 - val_acc: 0.7570\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9953 - acc: 0.7657 - val_loss: 1.0410 - val_acc: 0.7530\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9948 - acc: 0.7633 - val_loss: 1.0360 - val_acc: 0.7550\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9940 - acc: 0.7651 - val_loss: 1.0325 - val_acc: 0.7540\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9938 - acc: 0.7637 - val_loss: 1.0387 - val_acc: 0.7570\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9928 - acc: 0.7643 - val_loss: 1.0354 - val_acc: 0.7580\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9919 - acc: 0.7648 - val_loss: 1.0446 - val_acc: 0.7510\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9920 - acc: 0.7657 - val_loss: 1.0308 - val_acc: 0.7590\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9911 - acc: 0.7639 - val_loss: 1.0315 - val_acc: 0.7590\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9910 - acc: 0.7616 - val_loss: 1.0296 - val_acc: 0.7620\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9898 - acc: 0.7640 - val_loss: 1.0283 - val_acc: 0.7600\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9894 - acc: 0.7639 - val_loss: 1.0337 - val_acc: 0.7560\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9887 - acc: 0.7641 - val_loss: 1.0305 - val_acc: 0.7580\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9884 - acc: 0.7648 - val_loss: 1.0272 - val_acc: 0.7610\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9874 - acc: 0.7637 - val_loss: 1.0319 - val_acc: 0.7550\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9872 - acc: 0.7640 - val_loss: 1.0276 - val_acc: 0.7590\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9865 - acc: 0.7637 - val_loss: 1.0255 - val_acc: 0.7590\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9863 - acc: 0.7652 - val_loss: 1.0256 - val_acc: 0.7590\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9850 - acc: 0.7649 - val_loss: 1.0239 - val_acc: 0.7580\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9847 - acc: 0.7639 - val_loss: 1.0242 - val_acc: 0.7590\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9849 - acc: 0.7636 - val_loss: 1.0238 - val_acc: 0.7610\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9831 - acc: 0.7659 - val_loss: 1.0421 - val_acc: 0.7460\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9834 - acc: 0.7651 - val_loss: 1.0251 - val_acc: 0.7550\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9826 - acc: 0.7653 - val_loss: 1.0276 - val_acc: 0.7560\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9820 - acc: 0.7661 - val_loss: 1.0416 - val_acc: 0.7470\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9822 - acc: 0.7664 - val_loss: 1.0343 - val_acc: 0.7480\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9816 - acc: 0.7665 - val_loss: 1.0299 - val_acc: 0.7580\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9807 - acc: 0.7659 - val_loss: 1.0257 - val_acc: 0.7590\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9794 - acc: 0.7652 - val_loss: 1.0276 - val_acc: 0.7590\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9797 - acc: 0.7665 - val_loss: 1.0228 - val_acc: 0.7550\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9792 - acc: 0.7661 - val_loss: 1.0168 - val_acc: 0.7610\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9787 - acc: 0.7672 - val_loss: 1.0219 - val_acc: 0.7630\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9774 - acc: 0.7664 - val_loss: 1.0192 - val_acc: 0.7580\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9767 - acc: 0.7661 - val_loss: 1.0217 - val_acc: 0.7620\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9764 - acc: 0.7671 - val_loss: 1.0206 - val_acc: 0.7620\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9764 - acc: 0.7667 - val_loss: 1.0232 - val_acc: 0.7590\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9760 - acc: 0.7676 - val_loss: 1.0216 - val_acc: 0.7580\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9747 - acc: 0.7684 - val_loss: 1.0266 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9749 - acc: 0.7667 - val_loss: 1.0197 - val_acc: 0.7570\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9747 - acc: 0.7669 - val_loss: 1.0172 - val_acc: 0.7590\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9737 - acc: 0.7669 - val_loss: 1.0187 - val_acc: 0.7610\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9731 - acc: 0.7681 - val_loss: 1.0200 - val_acc: 0.7610\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9728 - acc: 0.7676 - val_loss: 1.0193 - val_acc: 0.7630\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9719 - acc: 0.7676 - val_loss: 1.0231 - val_acc: 0.7580\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9715 - acc: 0.7687 - val_loss: 1.0234 - val_acc: 0.7540\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9717 - acc: 0.7700 - val_loss: 1.0152 - val_acc: 0.7590\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9707 - acc: 0.7679 - val_loss: 1.0138 - val_acc: 0.7620\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9698 - acc: 0.7695 - val_loss: 1.0177 - val_acc: 0.7620\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9699 - acc: 0.7695 - val_loss: 1.0183 - val_acc: 0.7590\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9689 - acc: 0.7689 - val_loss: 1.0123 - val_acc: 0.7610\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9693 - acc: 0.7673 - val_loss: 1.0143 - val_acc: 0.7590\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9689 - acc: 0.7699 - val_loss: 1.0137 - val_acc: 0.7590\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9677 - acc: 0.7684 - val_loss: 1.0120 - val_acc: 0.7600\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9670 - acc: 0.7703 - val_loss: 1.0131 - val_acc: 0.7560\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9670 - acc: 0.7701 - val_loss: 1.0114 - val_acc: 0.7660\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9664 - acc: 0.7695 - val_loss: 1.0149 - val_acc: 0.7600\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9660 - acc: 0.7691 - val_loss: 1.0139 - val_acc: 0.7620\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9658 - acc: 0.7697 - val_loss: 1.0227 - val_acc: 0.7530\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9654 - acc: 0.7695 - val_loss: 1.0196 - val_acc: 0.7590\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9651 - acc: 0.7672 - val_loss: 1.0133 - val_acc: 0.7620\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9643 - acc: 0.7703 - val_loss: 1.0175 - val_acc: 0.7580\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9646 - acc: 0.7704 - val_loss: 1.0148 - val_acc: 0.7560\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9641 - acc: 0.7687 - val_loss: 1.0094 - val_acc: 0.7630\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9629 - acc: 0.7699 - val_loss: 1.0081 - val_acc: 0.7590\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9617 - acc: 0.7699 - val_loss: 1.0104 - val_acc: 0.7590\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9616 - acc: 0.7695 - val_loss: 1.0136 - val_acc: 0.7620\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9615 - acc: 0.7704 - val_loss: 1.0078 - val_acc: 0.7610\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9611 - acc: 0.7720 - val_loss: 1.0106 - val_acc: 0.7620\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9605 - acc: 0.7687 - val_loss: 1.0240 - val_acc: 0.7440\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9605 - acc: 0.7701 - val_loss: 1.0240 - val_acc: 0.7460\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9604 - acc: 0.7687 - val_loss: 1.0040 - val_acc: 0.7620\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9592 - acc: 0.7703 - val_loss: 1.0108 - val_acc: 0.7580\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9586 - acc: 0.7703 - val_loss: 1.0150 - val_acc: 0.7520\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9602 - acc: 0.7703 - val_loss: 1.0042 - val_acc: 0.7610\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9583 - acc: 0.7720 - val_loss: 1.0053 - val_acc: 0.7660\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9579 - acc: 0.7717 - val_loss: 1.0010 - val_acc: 0.7690\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9568 - acc: 0.7705 - val_loss: 1.0050 - val_acc: 0.7660\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9561 - acc: 0.7725 - val_loss: 1.0249 - val_acc: 0.7470\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9565 - acc: 0.7691 - val_loss: 1.0060 - val_acc: 0.7610\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9561 - acc: 0.7713 - val_loss: 1.0011 - val_acc: 0.7600\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9550 - acc: 0.7715 - val_loss: 1.0027 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9546 - acc: 0.7701 - val_loss: 1.0016 - val_acc: 0.7630\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9539 - acc: 0.7723 - val_loss: 1.0064 - val_acc: 0.7610\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9542 - acc: 0.7696 - val_loss: 1.0057 - val_acc: 0.7600\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9535 - acc: 0.7715 - val_loss: 1.0008 - val_acc: 0.7560\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9544 - acc: 0.7705 - val_loss: 0.9976 - val_acc: 0.7610\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9531 - acc: 0.7724 - val_loss: 1.0029 - val_acc: 0.7640\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9518 - acc: 0.7720 - val_loss: 1.0030 - val_acc: 0.7600\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9518 - acc: 0.7727 - val_loss: 1.0043 - val_acc: 0.7600\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9509 - acc: 0.7727 - val_loss: 1.0018 - val_acc: 0.7600\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9509 - acc: 0.7729 - val_loss: 0.9983 - val_acc: 0.7600\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9502 - acc: 0.7727 - val_loss: 0.9958 - val_acc: 0.7590\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9505 - acc: 0.7735 - val_loss: 1.0014 - val_acc: 0.7600\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9495 - acc: 0.7732 - val_loss: 0.9988 - val_acc: 0.7570\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9493 - acc: 0.7732 - val_loss: 1.0123 - val_acc: 0.7480\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9494 - acc: 0.7721 - val_loss: 0.9989 - val_acc: 0.7590\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9477 - acc: 0.7737 - val_loss: 0.9969 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9480 - acc: 0.7755 - val_loss: 0.9950 - val_acc: 0.7680\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9472 - acc: 0.7724 - val_loss: 0.9949 - val_acc: 0.7610\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9468 - acc: 0.7744 - val_loss: 1.0090 - val_acc: 0.7520\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9474 - acc: 0.7720 - val_loss: 0.9984 - val_acc: 0.7620\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9459 - acc: 0.7735 - val_loss: 1.0055 - val_acc: 0.7510\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9462 - acc: 0.7739 - val_loss: 0.9986 - val_acc: 0.7630\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9457 - acc: 0.7737 - val_loss: 1.0000 - val_acc: 0.7570\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9447 - acc: 0.7736 - val_loss: 0.9961 - val_acc: 0.7540\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9448 - acc: 0.7729 - val_loss: 0.9952 - val_acc: 0.7660\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9441 - acc: 0.7740 - val_loss: 0.9958 - val_acc: 0.7660\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9451 - acc: 0.7749 - val_loss: 0.9937 - val_acc: 0.7620\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9434 - acc: 0.7745 - val_loss: 0.9956 - val_acc: 0.7620\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9432 - acc: 0.7744 - val_loss: 0.9921 - val_acc: 0.7580\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9424 - acc: 0.7751 - val_loss: 0.9958 - val_acc: 0.7600\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9426 - acc: 0.7749 - val_loss: 1.0108 - val_acc: 0.7510\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9427 - acc: 0.7749 - val_loss: 0.9963 - val_acc: 0.7620\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9416 - acc: 0.7753 - val_loss: 0.9925 - val_acc: 0.7620\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9406 - acc: 0.7757 - val_loss: 0.9961 - val_acc: 0.7580\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9413 - acc: 0.7753 - val_loss: 0.9905 - val_acc: 0.7630\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9408 - acc: 0.7763 - val_loss: 1.0034 - val_acc: 0.7520\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9410 - acc: 0.7756 - val_loss: 0.9913 - val_acc: 0.7630\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9398 - acc: 0.7744 - val_loss: 0.9916 - val_acc: 0.7640\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9398 - acc: 0.7764 - val_loss: 0.9909 - val_acc: 0.7680\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9392 - acc: 0.7735 - val_loss: 0.9914 - val_acc: 0.7630\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9382 - acc: 0.7771 - val_loss: 0.9911 - val_acc: 0.7610\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9372 - acc: 0.7759 - val_loss: 0.9923 - val_acc: 0.7590\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9372 - acc: 0.7749 - val_loss: 0.9954 - val_acc: 0.7570\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9367 - acc: 0.7755 - val_loss: 0.9983 - val_acc: 0.7590\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9359 - acc: 0.7779 - val_loss: 0.9895 - val_acc: 0.7560\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9373 - acc: 0.7748 - val_loss: 0.9936 - val_acc: 0.7580\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9358 - acc: 0.7775 - val_loss: 0.9906 - val_acc: 0.7630\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9347 - acc: 0.7771 - val_loss: 0.9931 - val_acc: 0.7590\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9347 - acc: 0.7780 - val_loss: 0.9871 - val_acc: 0.7630\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9353 - acc: 0.7768 - val_loss: 0.9889 - val_acc: 0.7680\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9335 - acc: 0.7757 - val_loss: 0.9904 - val_acc: 0.7590\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9325 - acc: 0.7795 - val_loss: 0.9989 - val_acc: 0.7470\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9357 - acc: 0.7747 - val_loss: 0.9891 - val_acc: 0.7570\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9333 - acc: 0.7764 - val_loss: 0.9941 - val_acc: 0.7560\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9333 - acc: 0.7744 - val_loss: 0.9893 - val_acc: 0.7620\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9325 - acc: 0.7780 - val_loss: 0.9874 - val_acc: 0.7620\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9324 - acc: 0.7760 - val_loss: 0.9835 - val_acc: 0.7640\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9322 - acc: 0.7751 - val_loss: 0.9954 - val_acc: 0.7540\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9314 - acc: 0.7773 - val_loss: 0.9857 - val_acc: 0.7610\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9306 - acc: 0.7773 - val_loss: 0.9850 - val_acc: 0.7590\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9301 - acc: 0.7768 - val_loss: 0.9849 - val_acc: 0.7600\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9299 - acc: 0.7769 - val_loss: 0.9884 - val_acc: 0.7580\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9295 - acc: 0.7775 - val_loss: 0.9922 - val_acc: 0.7540\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9286 - acc: 0.7773 - val_loss: 0.9889 - val_acc: 0.7580\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9291 - acc: 0.7764 - val_loss: 0.9976 - val_acc: 0.7520\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9278 - acc: 0.7788 - val_loss: 0.9818 - val_acc: 0.7640\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9276 - acc: 0.7771 - val_loss: 0.9833 - val_acc: 0.7590\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9284 - acc: 0.7772 - val_loss: 0.9836 - val_acc: 0.7620\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9267 - acc: 0.7811 - val_loss: 0.9858 - val_acc: 0.7610\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9264 - acc: 0.7779 - val_loss: 0.9876 - val_acc: 0.7580\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9264 - acc: 0.7787 - val_loss: 0.9786 - val_acc: 0.7690\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9264 - acc: 0.7784 - val_loss: 0.9895 - val_acc: 0.7530\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9258 - acc: 0.7773 - val_loss: 0.9867 - val_acc: 0.7560\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9255 - acc: 0.7781 - val_loss: 0.9844 - val_acc: 0.7570\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9238 - acc: 0.7777 - val_loss: 0.9878 - val_acc: 0.7540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9241 - acc: 0.7785 - val_loss: 0.9845 - val_acc: 0.7620\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9234 - acc: 0.7787 - val_loss: 0.9925 - val_acc: 0.7490\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9227 - acc: 0.7792 - val_loss: 0.9862 - val_acc: 0.7580\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9230 - acc: 0.7783 - val_loss: 0.9760 - val_acc: 0.7630\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9221 - acc: 0.7769 - val_loss: 0.9779 - val_acc: 0.7630\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9226 - acc: 0.7773 - val_loss: 0.9803 - val_acc: 0.7630\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9220 - acc: 0.7796 - val_loss: 0.9845 - val_acc: 0.7540\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9218 - acc: 0.7784 - val_loss: 0.9862 - val_acc: 0.7530\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9222 - acc: 0.7773 - val_loss: 0.9868 - val_acc: 0.7620\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9207 - acc: 0.7809 - val_loss: 0.9854 - val_acc: 0.7590\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9206 - acc: 0.7785 - val_loss: 0.9751 - val_acc: 0.7640\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9199 - acc: 0.7771 - val_loss: 0.9757 - val_acc: 0.7670\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9195 - acc: 0.7799 - val_loss: 0.9796 - val_acc: 0.7620\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9188 - acc: 0.7785 - val_loss: 0.9941 - val_acc: 0.7560\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9200 - acc: 0.7776 - val_loss: 0.9760 - val_acc: 0.7580\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9177 - acc: 0.7791 - val_loss: 0.9728 - val_acc: 0.7640\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9180 - acc: 0.7795 - val_loss: 0.9784 - val_acc: 0.7620\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9181 - acc: 0.7775 - val_loss: 0.9731 - val_acc: 0.7660\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9163 - acc: 0.7789 - val_loss: 0.9772 - val_acc: 0.7570\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9168 - acc: 0.7785 - val_loss: 0.9748 - val_acc: 0.7650\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9172 - acc: 0.7796 - val_loss: 0.9756 - val_acc: 0.7590\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9162 - acc: 0.7792 - val_loss: 0.9819 - val_acc: 0.7570\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9167 - acc: 0.7800 - val_loss: 0.9731 - val_acc: 0.7610\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9153 - acc: 0.7812 - val_loss: 0.9869 - val_acc: 0.7500\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9168 - acc: 0.7800 - val_loss: 0.9787 - val_acc: 0.7600\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9115 - acc: 0.780 - 0s 43us/step - loss: 0.9143 - acc: 0.7805 - val_loss: 0.9803 - val_acc: 0.7570\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9152 - acc: 0.7803 - val_loss: 0.9734 - val_acc: 0.7600\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9138 - acc: 0.7813 - val_loss: 0.9871 - val_acc: 0.7530\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9132 - acc: 0.7801 - val_loss: 0.9696 - val_acc: 0.7630\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9129 - acc: 0.7804 - val_loss: 0.9695 - val_acc: 0.7620\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9122 - acc: 0.7816 - val_loss: 0.9748 - val_acc: 0.7600\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9119 - acc: 0.7819 - val_loss: 0.9962 - val_acc: 0.7470\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9135 - acc: 0.7792 - val_loss: 0.9797 - val_acc: 0.7550\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9105 - acc: 0.7816 - val_loss: 0.9687 - val_acc: 0.7610\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9116 - acc: 0.7800 - val_loss: 0.9709 - val_acc: 0.7590\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9104 - acc: 0.7811 - val_loss: 0.9675 - val_acc: 0.7660\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9103 - acc: 0.7789 - val_loss: 0.9767 - val_acc: 0.7550\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9094 - acc: 0.7811 - val_loss: 0.9724 - val_acc: 0.7610\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9100 - acc: 0.7805 - val_loss: 0.9711 - val_acc: 0.7580\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9085 - acc: 0.7803 - val_loss: 0.9806 - val_acc: 0.7550\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9093 - acc: 0.7796 - val_loss: 0.9701 - val_acc: 0.7630\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9084 - acc: 0.7807 - val_loss: 0.9684 - val_acc: 0.7590\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9076 - acc: 0.7813 - val_loss: 0.9670 - val_acc: 0.7610\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9078 - acc: 0.7813 - val_loss: 0.9692 - val_acc: 0.7600\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9073 - acc: 0.7816 - val_loss: 0.9659 - val_acc: 0.7610\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9075 - acc: 0.7812 - val_loss: 0.9678 - val_acc: 0.7620\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9058 - acc: 0.7807 - val_loss: 0.9704 - val_acc: 0.7610\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9059 - acc: 0.7820 - val_loss: 0.9723 - val_acc: 0.7610\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9060 - acc: 0.7823 - val_loss: 0.9691 - val_acc: 0.7570\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9055 - acc: 0.7815 - val_loss: 0.9652 - val_acc: 0.7590\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9052 - acc: 0.7800 - val_loss: 0.9733 - val_acc: 0.7570\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9044 - acc: 0.7820 - val_loss: 0.9747 - val_acc: 0.7530\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9042 - acc: 0.7809 - val_loss: 0.9690 - val_acc: 0.7560\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9053 - acc: 0.7808 - val_loss: 0.9837 - val_acc: 0.7490\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9052 - acc: 0.7816 - val_loss: 0.9627 - val_acc: 0.7620\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9029 - acc: 0.7817 - val_loss: 0.9719 - val_acc: 0.7550\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9027 - acc: 0.7836 - val_loss: 0.9631 - val_acc: 0.7600\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9025 - acc: 0.7808 - val_loss: 0.9758 - val_acc: 0.7590\n",
      "Epoch 531/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9039 - acc: 0.7808 - val_loss: 0.9744 - val_acc: 0.7550\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9023 - acc: 0.7835 - val_loss: 0.9672 - val_acc: 0.7590\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9010 - acc: 0.7829 - val_loss: 0.9807 - val_acc: 0.7490\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9014 - acc: 0.7815 - val_loss: 0.9599 - val_acc: 0.7640\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9004 - acc: 0.7817 - val_loss: 0.9664 - val_acc: 0.7580\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9008 - acc: 0.7839 - val_loss: 0.9606 - val_acc: 0.7620\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8994 - acc: 0.7833 - val_loss: 0.9605 - val_acc: 0.7630\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8996 - acc: 0.7843 - val_loss: 0.9714 - val_acc: 0.7560\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9008 - acc: 0.7825 - val_loss: 0.9798 - val_acc: 0.7480\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8987 - acc: 0.7840 - val_loss: 0.9625 - val_acc: 0.7610\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8977 - acc: 0.7820 - val_loss: 0.9689 - val_acc: 0.7550\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8970 - acc: 0.7840 - val_loss: 0.9608 - val_acc: 0.7640\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8969 - acc: 0.7835 - val_loss: 0.9612 - val_acc: 0.7580\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8968 - acc: 0.7836 - val_loss: 0.9646 - val_acc: 0.7590\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8958 - acc: 0.7845 - val_loss: 0.9676 - val_acc: 0.7540\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8976 - acc: 0.7841 - val_loss: 0.9891 - val_acc: 0.7520\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8969 - acc: 0.7823 - val_loss: 0.9661 - val_acc: 0.7550\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8951 - acc: 0.7840 - val_loss: 0.9608 - val_acc: 0.7580\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8959 - acc: 0.7821 - val_loss: 0.9605 - val_acc: 0.7560\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8961 - acc: 0.7832 - val_loss: 0.9658 - val_acc: 0.7560\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8947 - acc: 0.7837 - val_loss: 0.9691 - val_acc: 0.7540\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8937 - acc: 0.7845 - val_loss: 0.9655 - val_acc: 0.7570\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8939 - acc: 0.7819 - val_loss: 0.9578 - val_acc: 0.7610\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8946 - acc: 0.7843 - val_loss: 0.9578 - val_acc: 0.7580\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8937 - acc: 0.7835 - val_loss: 0.9641 - val_acc: 0.7590\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8932 - acc: 0.7840 - val_loss: 0.9650 - val_acc: 0.7530\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8930 - acc: 0.7827 - val_loss: 0.9611 - val_acc: 0.7520\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8919 - acc: 0.7833 - val_loss: 0.9678 - val_acc: 0.7560\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8922 - acc: 0.7849 - val_loss: 0.9630 - val_acc: 0.7560\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8927 - acc: 0.7852 - val_loss: 0.9591 - val_acc: 0.7540\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8905 - acc: 0.7832 - val_loss: 0.9607 - val_acc: 0.7600\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8915 - acc: 0.7832 - val_loss: 0.9732 - val_acc: 0.7530\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8910 - acc: 0.7844 - val_loss: 0.9600 - val_acc: 0.7610\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8905 - acc: 0.7859 - val_loss: 0.9563 - val_acc: 0.7620\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8897 - acc: 0.7837 - val_loss: 0.9585 - val_acc: 0.7600\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8900 - acc: 0.7837 - val_loss: 0.9660 - val_acc: 0.7540\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8884 - acc: 0.7852 - val_loss: 0.9617 - val_acc: 0.7580\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8888 - acc: 0.7836 - val_loss: 0.9557 - val_acc: 0.7590\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8880 - acc: 0.7853 - val_loss: 0.9560 - val_acc: 0.7600\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8879 - acc: 0.7844 - val_loss: 0.9623 - val_acc: 0.7530\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8873 - acc: 0.7853 - val_loss: 0.9653 - val_acc: 0.7520\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8891 - acc: 0.7856 - val_loss: 0.9520 - val_acc: 0.7620\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8873 - acc: 0.7863 - val_loss: 0.9708 - val_acc: 0.7500\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8874 - acc: 0.7855 - val_loss: 0.9548 - val_acc: 0.7600\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8858 - acc: 0.7844 - val_loss: 0.9508 - val_acc: 0.7610\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8848 - acc: 0.7845 - val_loss: 0.9587 - val_acc: 0.7570\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8860 - acc: 0.7857 - val_loss: 0.9678 - val_acc: 0.7490\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8841 - acc: 0.7860 - val_loss: 0.9517 - val_acc: 0.7580\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8841 - acc: 0.7855 - val_loss: 0.9515 - val_acc: 0.7580\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8838 - acc: 0.7857 - val_loss: 0.9556 - val_acc: 0.7590\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8845 - acc: 0.7875 - val_loss: 0.9607 - val_acc: 0.7570\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8835 - acc: 0.7835 - val_loss: 0.9558 - val_acc: 0.7560\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8833 - acc: 0.7855 - val_loss: 0.9717 - val_acc: 0.7500\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8844 - acc: 0.7851 - val_loss: 0.9620 - val_acc: 0.7510\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8819 - acc: 0.7852 - val_loss: 0.9620 - val_acc: 0.7560\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8832 - acc: 0.7845 - val_loss: 0.9538 - val_acc: 0.7580\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8809 - acc: 0.7877 - val_loss: 0.9543 - val_acc: 0.7550\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8809 - acc: 0.7877 - val_loss: 0.9605 - val_acc: 0.7540\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8814 - acc: 0.7861 - val_loss: 0.9559 - val_acc: 0.7540\n",
      "Epoch 590/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8828 - acc: 0.7845 - val_loss: 0.9509 - val_acc: 0.7570\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8808 - acc: 0.7857 - val_loss: 0.9567 - val_acc: 0.7590\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8802 - acc: 0.7864 - val_loss: 0.9616 - val_acc: 0.7550\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8813 - acc: 0.7856 - val_loss: 0.9557 - val_acc: 0.7530\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8799 - acc: 0.7837 - val_loss: 0.9510 - val_acc: 0.7560\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8785 - acc: 0.7861 - val_loss: 0.9522 - val_acc: 0.7580\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8793 - acc: 0.7879 - val_loss: 0.9629 - val_acc: 0.7540\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8789 - acc: 0.7865 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8781 - acc: 0.7868 - val_loss: 0.9479 - val_acc: 0.7600\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8780 - acc: 0.7867 - val_loss: 0.9498 - val_acc: 0.7590\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8771 - acc: 0.7863 - val_loss: 0.9476 - val_acc: 0.7610\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8754 - acc: 0.7877 - val_loss: 0.9729 - val_acc: 0.7500\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8787 - acc: 0.7860 - val_loss: 0.9536 - val_acc: 0.7600\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8762 - acc: 0.7869 - val_loss: 0.9563 - val_acc: 0.7530\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8748 - acc: 0.7876 - val_loss: 0.9517 - val_acc: 0.7560\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8756 - acc: 0.7871 - val_loss: 0.9512 - val_acc: 0.7560\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8758 - acc: 0.7875 - val_loss: 0.9540 - val_acc: 0.7540\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8755 - acc: 0.7859 - val_loss: 0.9518 - val_acc: 0.7550\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8746 - acc: 0.7877 - val_loss: 0.9529 - val_acc: 0.7500\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8740 - acc: 0.7873 - val_loss: 0.9473 - val_acc: 0.7570\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8735 - acc: 0.7876 - val_loss: 0.9444 - val_acc: 0.7570\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8733 - acc: 0.7868 - val_loss: 0.9674 - val_acc: 0.7560\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8759 - acc: 0.7881 - val_loss: 0.9579 - val_acc: 0.7480\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8735 - acc: 0.7877 - val_loss: 0.9414 - val_acc: 0.7620\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8713 - acc: 0.7868 - val_loss: 0.9479 - val_acc: 0.7580\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8724 - acc: 0.7876 - val_loss: 0.9528 - val_acc: 0.7550\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8719 - acc: 0.7867 - val_loss: 0.9465 - val_acc: 0.7590\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8704 - acc: 0.7872 - val_loss: 0.9474 - val_acc: 0.7550\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8714 - acc: 0.7883 - val_loss: 0.9688 - val_acc: 0.7560\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8732 - acc: 0.7871 - val_loss: 0.9638 - val_acc: 0.7510\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8719 - acc: 0.7868 - val_loss: 0.9564 - val_acc: 0.7510\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8701 - acc: 0.7897 - val_loss: 0.9472 - val_acc: 0.7580\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8691 - acc: 0.7877 - val_loss: 0.9475 - val_acc: 0.7560\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8692 - acc: 0.7875 - val_loss: 0.9454 - val_acc: 0.7570\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8700 - acc: 0.7881 - val_loss: 0.9694 - val_acc: 0.7470\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8688 - acc: 0.7880 - val_loss: 0.9549 - val_acc: 0.7560\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8691 - acc: 0.7891 - val_loss: 0.9515 - val_acc: 0.7530\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8677 - acc: 0.7880 - val_loss: 0.9544 - val_acc: 0.7510\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8662 - acc: 0.7907 - val_loss: 0.9493 - val_acc: 0.7500\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8680 - acc: 0.7887 - val_loss: 0.9456 - val_acc: 0.7570\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8664 - acc: 0.7899 - val_loss: 0.9509 - val_acc: 0.7570\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8676 - acc: 0.7896 - val_loss: 0.9510 - val_acc: 0.7480\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8685 - acc: 0.7864 - val_loss: 0.9425 - val_acc: 0.7580\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8663 - acc: 0.7881 - val_loss: 0.9485 - val_acc: 0.7540\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8661 - acc: 0.7907 - val_loss: 0.9407 - val_acc: 0.7570\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8660 - acc: 0.7877 - val_loss: 0.9471 - val_acc: 0.7560\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8655 - acc: 0.7876 - val_loss: 0.9444 - val_acc: 0.7550\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8644 - acc: 0.7889 - val_loss: 0.9442 - val_acc: 0.7570\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8638 - acc: 0.7889 - val_loss: 0.9387 - val_acc: 0.7590\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8645 - acc: 0.7887 - val_loss: 0.9485 - val_acc: 0.7530\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8634 - acc: 0.7912 - val_loss: 0.9457 - val_acc: 0.7540\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8641 - acc: 0.7885 - val_loss: 0.9450 - val_acc: 0.7580\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8641 - acc: 0.7887 - val_loss: 0.9418 - val_acc: 0.7560\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8621 - acc: 0.7887 - val_loss: 0.9492 - val_acc: 0.7570\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8634 - acc: 0.7881 - val_loss: 0.9391 - val_acc: 0.7570\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8624 - acc: 0.7885 - val_loss: 0.9487 - val_acc: 0.7540\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8633 - acc: 0.7887 - val_loss: 0.9584 - val_acc: 0.7460\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8610 - acc: 0.7911 - val_loss: 0.9445 - val_acc: 0.7550\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8615 - acc: 0.7889 - val_loss: 0.9452 - val_acc: 0.7560\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8608 - acc: 0.7895 - val_loss: 0.9558 - val_acc: 0.7490\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8615 - acc: 0.7869 - val_loss: 0.9453 - val_acc: 0.7500\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8596 - acc: 0.7895 - val_loss: 0.9567 - val_acc: 0.7570\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8612 - acc: 0.7911 - val_loss: 0.9433 - val_acc: 0.7530\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8586 - acc: 0.7915 - val_loss: 0.9367 - val_acc: 0.7600\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8593 - acc: 0.7896 - val_loss: 0.9379 - val_acc: 0.7580\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8596 - acc: 0.7897 - val_loss: 0.9417 - val_acc: 0.7580\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8590 - acc: 0.7884 - val_loss: 0.9386 - val_acc: 0.7580\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8581 - acc: 0.7905 - val_loss: 0.9553 - val_acc: 0.7490\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8583 - acc: 0.7904 - val_loss: 0.9410 - val_acc: 0.7600\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8573 - acc: 0.7896 - val_loss: 0.9446 - val_acc: 0.7530\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8580 - acc: 0.7904 - val_loss: 0.9442 - val_acc: 0.7580\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8574 - acc: 0.7900 - val_loss: 0.9377 - val_acc: 0.7530\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8555 - acc: 0.7916 - val_loss: 0.9405 - val_acc: 0.7550\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8559 - acc: 0.7916 - val_loss: 0.9380 - val_acc: 0.7550\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8558 - acc: 0.7905 - val_loss: 0.9430 - val_acc: 0.7500\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8559 - acc: 0.7923 - val_loss: 0.9374 - val_acc: 0.7600\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8566 - acc: 0.7895 - val_loss: 0.9523 - val_acc: 0.7510\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8549 - acc: 0.7923 - val_loss: 0.9367 - val_acc: 0.7540\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8546 - acc: 0.7900 - val_loss: 0.9372 - val_acc: 0.7570\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8552 - acc: 0.7891 - val_loss: 0.9381 - val_acc: 0.7580\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8546 - acc: 0.7893 - val_loss: 0.9397 - val_acc: 0.7570\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8538 - acc: 0.7909 - val_loss: 0.9428 - val_acc: 0.7550\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8541 - acc: 0.7916 - val_loss: 0.9614 - val_acc: 0.7480\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8534 - acc: 0.7911 - val_loss: 0.9442 - val_acc: 0.7550\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8534 - acc: 0.7916 - val_loss: 0.9440 - val_acc: 0.7520\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8533 - acc: 0.7913 - val_loss: 0.9345 - val_acc: 0.7600\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8529 - acc: 0.7923 - val_loss: 0.9377 - val_acc: 0.7560\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8527 - acc: 0.7913 - val_loss: 0.9491 - val_acc: 0.7600\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8539 - acc: 0.7911 - val_loss: 0.9321 - val_acc: 0.7640\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8512 - acc: 0.7895 - val_loss: 0.9399 - val_acc: 0.7560\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8511 - acc: 0.7912 - val_loss: 0.9451 - val_acc: 0.7500\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8519 - acc: 0.7917 - val_loss: 0.9293 - val_acc: 0.7590\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8506 - acc: 0.7909 - val_loss: 0.9356 - val_acc: 0.7570\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8537 - acc: 0.7900 - val_loss: 0.9328 - val_acc: 0.7600\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8506 - acc: 0.7929 - val_loss: 0.9412 - val_acc: 0.7570\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8532 - acc: 0.7912 - val_loss: 0.9502 - val_acc: 0.7530\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8517 - acc: 0.7916 - val_loss: 0.9470 - val_acc: 0.7540\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8503 - acc: 0.7900 - val_loss: 0.9310 - val_acc: 0.7600\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8486 - acc: 0.7916 - val_loss: 0.9400 - val_acc: 0.7570\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8514 - acc: 0.7917 - val_loss: 0.9470 - val_acc: 0.7500\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8493 - acc: 0.7933 - val_loss: 0.9329 - val_acc: 0.7560\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8486 - acc: 0.7920 - val_loss: 0.9363 - val_acc: 0.7550\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8496 - acc: 0.7900 - val_loss: 0.9403 - val_acc: 0.7580\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8493 - acc: 0.7921 - val_loss: 0.9411 - val_acc: 0.7500\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8483 - acc: 0.7915 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8496 - acc: 0.7919 - val_loss: 0.9407 - val_acc: 0.7590\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8490 - acc: 0.7928 - val_loss: 0.9318 - val_acc: 0.7570\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8486 - acc: 0.7927 - val_loss: 0.9321 - val_acc: 0.7590\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8478 - acc: 0.7935 - val_loss: 0.9352 - val_acc: 0.7590\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8466 - acc: 0.7940 - val_loss: 0.9365 - val_acc: 0.7560\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8479 - acc: 0.7917 - val_loss: 0.9489 - val_acc: 0.7530\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8476 - acc: 0.7923 - val_loss: 0.9415 - val_acc: 0.7540\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8463 - acc: 0.7929 - val_loss: 0.9286 - val_acc: 0.7620\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8443 - acc: 0.7952 - val_loss: 0.9347 - val_acc: 0.7600\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8469 - acc: 0.7920 - val_loss: 0.9454 - val_acc: 0.7520\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8460 - acc: 0.7907 - val_loss: 0.9363 - val_acc: 0.7520\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8457 - acc: 0.7923 - val_loss: 0.9357 - val_acc: 0.7530\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8451 - acc: 0.7897 - val_loss: 0.9395 - val_acc: 0.7560\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8447 - acc: 0.7913 - val_loss: 0.9427 - val_acc: 0.7610\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8457 - acc: 0.7923 - val_loss: 0.9455 - val_acc: 0.7510\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8457 - acc: 0.7925 - val_loss: 0.9368 - val_acc: 0.7530\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8447 - acc: 0.7937 - val_loss: 0.9345 - val_acc: 0.7590\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8452 - acc: 0.7932 - val_loss: 0.9282 - val_acc: 0.7590\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8438 - acc: 0.7943 - val_loss: 0.9299 - val_acc: 0.7650\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8449 - acc: 0.7915 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8441 - acc: 0.7936 - val_loss: 0.9284 - val_acc: 0.7620\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8422 - acc: 0.7925 - val_loss: 0.9337 - val_acc: 0.7580\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8436 - acc: 0.7928 - val_loss: 0.9335 - val_acc: 0.7560\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8427 - acc: 0.7923 - val_loss: 0.9362 - val_acc: 0.7600\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8420 - acc: 0.7927 - val_loss: 0.9251 - val_acc: 0.7610\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8420 - acc: 0.7941 - val_loss: 0.9414 - val_acc: 0.7510\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8413 - acc: 0.7943 - val_loss: 0.9441 - val_acc: 0.7480\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8428 - acc: 0.7923 - val_loss: 0.9250 - val_acc: 0.7620\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8416 - acc: 0.7936 - val_loss: 0.9274 - val_acc: 0.7630\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8411 - acc: 0.7941 - val_loss: 0.9285 - val_acc: 0.7600\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8409 - acc: 0.7948 - val_loss: 0.9566 - val_acc: 0.7490\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8409 - acc: 0.7931 - val_loss: 0.9271 - val_acc: 0.7600\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8420 - acc: 0.7951 - val_loss: 0.9307 - val_acc: 0.7610\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8400 - acc: 0.7939 - val_loss: 0.9298 - val_acc: 0.7550\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8400 - acc: 0.7933 - val_loss: 0.9403 - val_acc: 0.7590\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8401 - acc: 0.7920 - val_loss: 0.9269 - val_acc: 0.7610\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8386 - acc: 0.7939 - val_loss: 0.9289 - val_acc: 0.7580\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8392 - acc: 0.7948 - val_loss: 0.9298 - val_acc: 0.7580\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8385 - acc: 0.7935 - val_loss: 0.9401 - val_acc: 0.7520\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8394 - acc: 0.7925 - val_loss: 0.9283 - val_acc: 0.7620\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8385 - acc: 0.7949 - val_loss: 0.9293 - val_acc: 0.7620\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8383 - acc: 0.7917 - val_loss: 0.9514 - val_acc: 0.7510\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8392 - acc: 0.7940 - val_loss: 0.9474 - val_acc: 0.7540\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8397 - acc: 0.7937 - val_loss: 0.9329 - val_acc: 0.7530\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8377 - acc: 0.7952 - val_loss: 0.9295 - val_acc: 0.7590\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8365 - acc: 0.7927 - val_loss: 0.9258 - val_acc: 0.7600\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8365 - acc: 0.7947 - val_loss: 0.9367 - val_acc: 0.7530\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8389 - acc: 0.7932 - val_loss: 0.9326 - val_acc: 0.7560\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8374 - acc: 0.7936 - val_loss: 0.9414 - val_acc: 0.7520\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8372 - acc: 0.7943 - val_loss: 0.9517 - val_acc: 0.7530\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8368 - acc: 0.7964 - val_loss: 0.9299 - val_acc: 0.7520\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8361 - acc: 0.7937 - val_loss: 0.9365 - val_acc: 0.7540\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8357 - acc: 0.7949 - val_loss: 0.9299 - val_acc: 0.7590\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8359 - acc: 0.7963 - val_loss: 0.9281 - val_acc: 0.7610\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8361 - acc: 0.7949 - val_loss: 0.9296 - val_acc: 0.7570\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8359 - acc: 0.7935 - val_loss: 0.9487 - val_acc: 0.7520\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8365 - acc: 0.7941 - val_loss: 0.9386 - val_acc: 0.7570\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8343 - acc: 0.7947 - val_loss: 0.9280 - val_acc: 0.7560\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8356 - acc: 0.7947 - val_loss: 0.9278 - val_acc: 0.7590\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8334 - acc: 0.7957 - val_loss: 0.9303 - val_acc: 0.7590\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8328 - acc: 0.7952 - val_loss: 0.9494 - val_acc: 0.7460\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8345 - acc: 0.7939 - val_loss: 0.9430 - val_acc: 0.7570\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8348 - acc: 0.7928 - val_loss: 0.9282 - val_acc: 0.7610\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8343 - acc: 0.7964 - val_loss: 0.9277 - val_acc: 0.7570\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8337 - acc: 0.7951 - val_loss: 0.9263 - val_acc: 0.7620\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8333 - acc: 0.7936 - val_loss: 0.9568 - val_acc: 0.7480\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8335 - acc: 0.7959 - val_loss: 0.9337 - val_acc: 0.7550\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8335 - acc: 0.7941 - val_loss: 0.9338 - val_acc: 0.7540\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8320 - acc: 0.7957 - val_loss: 0.9325 - val_acc: 0.7570\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8346 - acc: 0.7949 - val_loss: 0.9277 - val_acc: 0.7620\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8328 - acc: 0.7941 - val_loss: 0.9495 - val_acc: 0.7570\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8337 - acc: 0.7939 - val_loss: 0.9475 - val_acc: 0.7510\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8313 - acc: 0.7961 - val_loss: 0.9261 - val_acc: 0.7620\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8306 - acc: 0.7961 - val_loss: 0.9255 - val_acc: 0.7630\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8306 - acc: 0.7965 - val_loss: 0.9341 - val_acc: 0.7550\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8323 - acc: 0.7968 - val_loss: 0.9309 - val_acc: 0.7610\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8298 - acc: 0.7960 - val_loss: 0.9370 - val_acc: 0.7560\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8319 - acc: 0.7940 - val_loss: 0.9287 - val_acc: 0.7550\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8305 - acc: 0.7952 - val_loss: 0.9187 - val_acc: 0.7590\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8290 - acc: 0.7975 - val_loss: 0.9258 - val_acc: 0.7600\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8302 - acc: 0.7959 - val_loss: 0.9363 - val_acc: 0.7560\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8284 - acc: 0.7967 - val_loss: 0.9298 - val_acc: 0.7550\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8280 - acc: 0.7956 - val_loss: 0.9191 - val_acc: 0.7600\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8299 - acc: 0.7961 - val_loss: 0.9374 - val_acc: 0.7560\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8307 - acc: 0.7963 - val_loss: 0.9265 - val_acc: 0.7560\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8300 - acc: 0.7952 - val_loss: 0.9357 - val_acc: 0.7570\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8285 - acc: 0.7953 - val_loss: 0.9193 - val_acc: 0.7640\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8305 - acc: 0.7951 - val_loss: 0.9367 - val_acc: 0.7590\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8287 - acc: 0.7960 - val_loss: 0.9281 - val_acc: 0.7560\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8298 - acc: 0.7949 - val_loss: 0.9257 - val_acc: 0.7610\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8283 - acc: 0.7971 - val_loss: 0.9206 - val_acc: 0.7570\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8286 - acc: 0.7955 - val_loss: 0.9219 - val_acc: 0.7610\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8288 - acc: 0.7949 - val_loss: 0.9230 - val_acc: 0.7620\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8271 - acc: 0.7973 - val_loss: 0.9224 - val_acc: 0.7600\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8276 - acc: 0.7944 - val_loss: 0.9472 - val_acc: 0.7530\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.7969 - val_loss: 0.9168 - val_acc: 0.7650\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8261 - acc: 0.7968 - val_loss: 0.9306 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8280 - acc: 0.7971 - val_loss: 0.9236 - val_acc: 0.7560\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8263 - acc: 0.7957 - val_loss: 0.9168 - val_acc: 0.7630\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8260 - acc: 0.7976 - val_loss: 0.9316 - val_acc: 0.7560\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8259 - acc: 0.7991 - val_loss: 0.9263 - val_acc: 0.7590\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8267 - acc: 0.7976 - val_loss: 0.9314 - val_acc: 0.7580\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8254 - acc: 0.7981 - val_loss: 0.9284 - val_acc: 0.7610\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8253 - acc: 0.7967 - val_loss: 0.9395 - val_acc: 0.7560\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8267 - acc: 0.7955 - val_loss: 0.9319 - val_acc: 0.7580\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8273 - acc: 0.7987 - val_loss: 0.9344 - val_acc: 0.7570\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8256 - acc: 0.7956 - val_loss: 0.9470 - val_acc: 0.7530\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8270 - acc: 0.7977 - val_loss: 0.9259 - val_acc: 0.7570\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8234 - acc: 0.7992 - val_loss: 0.9435 - val_acc: 0.7540\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8252 - acc: 0.7963 - val_loss: 0.9345 - val_acc: 0.7550\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8250 - acc: 0.7961 - val_loss: 0.9245 - val_acc: 0.7610\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8236 - acc: 0.7971 - val_loss: 0.9285 - val_acc: 0.7540\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8242 - acc: 0.7988 - val_loss: 0.9297 - val_acc: 0.7550\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8249 - acc: 0.7956 - val_loss: 0.9298 - val_acc: 0.7570\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8234 - acc: 0.7964 - val_loss: 0.9400 - val_acc: 0.7550\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8250 - acc: 0.7959 - val_loss: 0.9292 - val_acc: 0.7600\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8247 - acc: 0.7976 - val_loss: 0.9191 - val_acc: 0.7590\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8231 - acc: 0.7980 - val_loss: 0.9390 - val_acc: 0.7550\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8244 - acc: 0.7963 - val_loss: 0.9222 - val_acc: 0.7590\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8239 - acc: 0.7988 - val_loss: 0.9208 - val_acc: 0.7590\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8214 - acc: 0.7983 - val_loss: 0.9385 - val_acc: 0.7490\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8232 - acc: 0.7987 - val_loss: 0.9173 - val_acc: 0.7550\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8212 - acc: 0.8011 - val_loss: 0.9478 - val_acc: 0.7560\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8252 - acc: 0.7975 - val_loss: 0.9641 - val_acc: 0.7430\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8247 - acc: 0.7936 - val_loss: 0.9301 - val_acc: 0.7550\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8216 - acc: 0.7987 - val_loss: 0.9640 - val_acc: 0.7500\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8218 - acc: 0.7988 - val_loss: 0.9316 - val_acc: 0.7590\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8222 - acc: 0.7983 - val_loss: 0.9199 - val_acc: 0.7570\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8232 - acc: 0.7989 - val_loss: 0.9162 - val_acc: 0.7640\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8200 - acc: 0.7969 - val_loss: 0.9254 - val_acc: 0.7580\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8207 - acc: 0.7979 - val_loss: 0.9150 - val_acc: 0.7610\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8204 - acc: 0.7977 - val_loss: 0.9187 - val_acc: 0.7580\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8201 - acc: 0.7965 - val_loss: 0.9179 - val_acc: 0.7580\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8197 - acc: 0.7996 - val_loss: 0.9424 - val_acc: 0.7540\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8207 - acc: 0.7976 - val_loss: 0.9187 - val_acc: 0.7550\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8197 - acc: 0.7980 - val_loss: 0.9187 - val_acc: 0.7610\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8191 - acc: 0.7977 - val_loss: 0.9462 - val_acc: 0.7490\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8203 - acc: 0.7993 - val_loss: 0.9146 - val_acc: 0.7590\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8205 - acc: 0.7980 - val_loss: 0.9141 - val_acc: 0.7590\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8203 - acc: 0.7951 - val_loss: 0.9166 - val_acc: 0.7620\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8190 - acc: 0.7983 - val_loss: 0.9489 - val_acc: 0.7470\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8196 - acc: 0.7968 - val_loss: 0.9299 - val_acc: 0.7510\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8204 - acc: 0.7992 - val_loss: 0.9352 - val_acc: 0.7570\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8184 - acc: 0.7981 - val_loss: 0.9388 - val_acc: 0.7540\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8199 - acc: 0.7987 - val_loss: 0.9274 - val_acc: 0.7560\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8202 - acc: 0.7971 - val_loss: 0.9217 - val_acc: 0.7580\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8193 - acc: 0.7983 - val_loss: 0.9249 - val_acc: 0.7610\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8182 - acc: 0.7996 - val_loss: 0.9162 - val_acc: 0.7590\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8175 - acc: 0.7972 - val_loss: 0.9175 - val_acc: 0.7600\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8188 - acc: 0.7969 - val_loss: 0.9163 - val_acc: 0.7610\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8166 - acc: 0.7983 - val_loss: 0.9261 - val_acc: 0.7610\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8191 - acc: 0.7992 - val_loss: 0.9230 - val_acc: 0.7540\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8188 - acc: 0.7976 - val_loss: 0.9206 - val_acc: 0.7590\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8184 - acc: 0.7991 - val_loss: 0.9239 - val_acc: 0.7590\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8167 - acc: 0.7984 - val_loss: 0.9321 - val_acc: 0.7560\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8201 - acc: 0.7981 - val_loss: 0.9159 - val_acc: 0.7620\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8171 - acc: 0.7991 - val_loss: 0.9203 - val_acc: 0.7560\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8156 - acc: 0.8004 - val_loss: 0.9158 - val_acc: 0.7540\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8154 - acc: 0.7996 - val_loss: 0.9194 - val_acc: 0.7570\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8175 - acc: 0.7965 - val_loss: 0.9246 - val_acc: 0.7510\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8176 - acc: 0.7955 - val_loss: 0.9150 - val_acc: 0.7620\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8168 - acc: 0.7985 - val_loss: 0.9211 - val_acc: 0.7610\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8158 - acc: 0.7987 - val_loss: 0.9249 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8154 - acc: 0.7971 - val_loss: 0.9511 - val_acc: 0.7530\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8166 - acc: 0.7976 - val_loss: 0.9295 - val_acc: 0.7570\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8166 - acc: 0.7981 - val_loss: 0.9339 - val_acc: 0.7580\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8163 - acc: 0.7985 - val_loss: 0.9134 - val_acc: 0.7610\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8154 - acc: 0.7979 - val_loss: 0.9177 - val_acc: 0.7610\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8145 - acc: 0.7999 - val_loss: 0.9202 - val_acc: 0.7540\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8153 - acc: 0.7980 - val_loss: 0.9283 - val_acc: 0.7590\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8143 - acc: 0.8013 - val_loss: 0.9462 - val_acc: 0.7490\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8143 - acc: 0.8003 - val_loss: 0.9245 - val_acc: 0.7600\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8158 - acc: 0.8005 - val_loss: 0.9236 - val_acc: 0.7580\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8157 - acc: 0.8003 - val_loss: 0.9212 - val_acc: 0.7590\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8140 - acc: 0.7973 - val_loss: 0.9206 - val_acc: 0.7590\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8150 - acc: 0.7993 - val_loss: 0.9264 - val_acc: 0.7620\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8156 - acc: 0.7985 - val_loss: 0.9146 - val_acc: 0.7640\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8133 - acc: 0.7981 - val_loss: 0.9141 - val_acc: 0.7560\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8152 - acc: 0.7984 - val_loss: 0.9129 - val_acc: 0.7590\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8151 - acc: 0.7996 - val_loss: 0.9113 - val_acc: 0.7610\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8122 - acc: 0.8001 - val_loss: 0.9087 - val_acc: 0.7620\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8153 - acc: 0.7964 - val_loss: 0.9235 - val_acc: 0.7590\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8136 - acc: 0.7995 - val_loss: 0.9164 - val_acc: 0.7620\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8138 - acc: 0.7989 - val_loss: 0.9215 - val_acc: 0.7560\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8130 - acc: 0.8001 - val_loss: 0.9306 - val_acc: 0.7530\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8128 - acc: 0.7985 - val_loss: 0.9148 - val_acc: 0.7620\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8137 - acc: 0.7989 - val_loss: 0.9391 - val_acc: 0.7520\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8144 - acc: 0.7961 - val_loss: 0.9591 - val_acc: 0.7470\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8135 - acc: 0.7985 - val_loss: 0.9197 - val_acc: 0.7630\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8136 - acc: 0.7988 - val_loss: 0.9214 - val_acc: 0.7580\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8132 - acc: 0.7983 - val_loss: 0.9228 - val_acc: 0.7630\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8124 - acc: 0.8007 - val_loss: 0.9381 - val_acc: 0.7500\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8144 - acc: 0.7997 - val_loss: 0.9117 - val_acc: 0.7570\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8130 - acc: 0.7983 - val_loss: 0.9191 - val_acc: 0.7560\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8126 - acc: 0.7984 - val_loss: 0.9235 - val_acc: 0.7530\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8117 - acc: 0.7993 - val_loss: 0.9221 - val_acc: 0.7620\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8123 - acc: 0.7999 - val_loss: 0.9298 - val_acc: 0.7550\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8130 - acc: 0.7997 - val_loss: 0.9296 - val_acc: 0.7610\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8117 - acc: 0.8008 - val_loss: 0.9160 - val_acc: 0.7560\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8110 - acc: 0.8001 - val_loss: 0.9209 - val_acc: 0.7540\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8122 - acc: 0.7981 - val_loss: 0.9562 - val_acc: 0.7610\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8160 - acc: 0.7973 - val_loss: 0.9229 - val_acc: 0.7520\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8121 - acc: 0.7996 - val_loss: 0.9326 - val_acc: 0.7520\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8111 - acc: 0.8001 - val_loss: 0.9243 - val_acc: 0.7660\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8115 - acc: 0.7996 - val_loss: 0.9215 - val_acc: 0.7630\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8105 - acc: 0.8003 - val_loss: 0.9164 - val_acc: 0.7560\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8095 - acc: 0.7981 - val_loss: 0.9241 - val_acc: 0.7520\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8089 - acc: 0.7999 - val_loss: 0.9186 - val_acc: 0.7580\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8113 - acc: 0.7996 - val_loss: 0.9155 - val_acc: 0.7550\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8100 - acc: 0.8025 - val_loss: 0.9186 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8110 - acc: 0.7997 - val_loss: 0.9097 - val_acc: 0.7620\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8091 - acc: 0.7996 - val_loss: 0.9118 - val_acc: 0.7540\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8099 - acc: 0.7995 - val_loss: 0.9679 - val_acc: 0.7470\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8091 - acc: 0.7999 - val_loss: 0.9316 - val_acc: 0.7620\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8099 - acc: 0.7985 - val_loss: 0.9293 - val_acc: 0.7600\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8106 - acc: 0.7997 - val_loss: 0.9681 - val_acc: 0.7370\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8144 - acc: 0.7987 - val_loss: 0.9135 - val_acc: 0.7580\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8092 - acc: 0.8007 - val_loss: 0.9278 - val_acc: 0.7580\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8101 - acc: 0.8001 - val_loss: 0.9196 - val_acc: 0.7540\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8123 - acc: 0.7979 - val_loss: 0.9547 - val_acc: 0.7470\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8116 - acc: 0.8012 - val_loss: 0.9178 - val_acc: 0.7620\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8096 - acc: 0.7993 - val_loss: 0.9078 - val_acc: 0.7560\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8101 - acc: 0.8001 - val_loss: 0.9076 - val_acc: 0.7560\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8079 - acc: 0.7991 - val_loss: 0.9282 - val_acc: 0.7640\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8085 - acc: 0.8007 - val_loss: 0.9340 - val_acc: 0.7520\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8078 - acc: 0.7997 - val_loss: 0.9275 - val_acc: 0.7550\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8081 - acc: 0.7991 - val_loss: 0.9111 - val_acc: 0.7550\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8074 - acc: 0.8005 - val_loss: 0.9165 - val_acc: 0.7620\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8074 - acc: 0.8005 - val_loss: 0.9406 - val_acc: 0.7480\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8085 - acc: 0.8013 - val_loss: 0.9118 - val_acc: 0.7600\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8064 - acc: 0.8024 - val_loss: 0.9326 - val_acc: 0.7490\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8084 - acc: 0.7984 - val_loss: 0.9382 - val_acc: 0.7560\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8070 - acc: 0.7977 - val_loss: 0.9182 - val_acc: 0.7660\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8101 - acc: 0.8029 - val_loss: 0.9187 - val_acc: 0.7620\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8077 - acc: 0.8012 - val_loss: 0.9203 - val_acc: 0.7560\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8074 - acc: 0.8011 - val_loss: 0.9389 - val_acc: 0.7500\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8071 - acc: 0.8024 - val_loss: 0.9314 - val_acc: 0.7480\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8074 - acc: 0.8005 - val_loss: 0.9330 - val_acc: 0.7540\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8060 - acc: 0.8008 - val_loss: 0.9546 - val_acc: 0.7480\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8067 - acc: 0.8027 - val_loss: 0.9377 - val_acc: 0.7510\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8062 - acc: 0.8009 - val_loss: 0.9284 - val_acc: 0.7590\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8088 - acc: 0.8000 - val_loss: 0.9467 - val_acc: 0.7520\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8054 - acc: 0.8021 - val_loss: 0.9135 - val_acc: 0.7600\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8058 - acc: 0.8005 - val_loss: 0.9181 - val_acc: 0.7620\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8058 - acc: 0.8007 - val_loss: 0.9110 - val_acc: 0.7590\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8065 - acc: 0.8024 - val_loss: 0.9405 - val_acc: 0.7470\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8080 - acc: 0.8005 - val_loss: 0.9107 - val_acc: 0.7630\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8044 - acc: 0.8009 - val_loss: 0.9182 - val_acc: 0.7630\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8039 - acc: 0.8011 - val_loss: 0.9434 - val_acc: 0.7520\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8059 - acc: 0.8037 - val_loss: 0.9219 - val_acc: 0.7560\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8057 - acc: 0.8019 - val_loss: 0.9130 - val_acc: 0.7600\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8056 - acc: 0.8019 - val_loss: 0.9648 - val_acc: 0.7500\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8079 - acc: 0.7999 - val_loss: 0.9223 - val_acc: 0.7530\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8051 - acc: 0.8029 - val_loss: 0.9305 - val_acc: 0.7510\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8069 - acc: 0.8007 - val_loss: 0.9407 - val_acc: 0.7540\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8044 - acc: 0.8017 - val_loss: 0.9651 - val_acc: 0.7410\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8081 - acc: 0.8003 - val_loss: 0.9849 - val_acc: 0.7420\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8044 - acc: 0.8020 - val_loss: 0.9206 - val_acc: 0.7560\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8054 - acc: 0.8035 - val_loss: 0.9603 - val_acc: 0.7480\n",
      "Epoch 954/1000\n",
      "4608/7500 [=================>............] - ETA: 0s - loss: 0.7998 - acc: 0.8030"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8237653533299764, 0.7967999999682108]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.966706668694814, 0.7499999998410543]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 2.0228 - acc: 0.1372 - val_loss: 1.9610 - val_acc: 0.1380\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9760 - acc: 0.1439 - val_loss: 1.9430 - val_acc: 0.1630\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9560 - acc: 0.1548 - val_loss: 1.9328 - val_acc: 0.1790\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9462 - acc: 0.1639 - val_loss: 1.9249 - val_acc: 0.1890\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9412 - acc: 0.1661 - val_loss: 1.9189 - val_acc: 0.2040\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9261 - acc: 0.1800 - val_loss: 1.9124 - val_acc: 0.2090\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9231 - acc: 0.1896 - val_loss: 1.9062 - val_acc: 0.2310\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9152 - acc: 0.1916 - val_loss: 1.8993 - val_acc: 0.2430\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9105 - acc: 0.1981 - val_loss: 1.8928 - val_acc: 0.2540\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9032 - acc: 0.2036 - val_loss: 1.8854 - val_acc: 0.2590\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8927 - acc: 0.2129 - val_loss: 1.8773 - val_acc: 0.2640\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8881 - acc: 0.2221 - val_loss: 1.8688 - val_acc: 0.2660\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8835 - acc: 0.2249 - val_loss: 1.8599 - val_acc: 0.2720\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8740 - acc: 0.2337 - val_loss: 1.8489 - val_acc: 0.2840\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8637 - acc: 0.2427 - val_loss: 1.8371 - val_acc: 0.2930\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8544 - acc: 0.2432 - val_loss: 1.8235 - val_acc: 0.3030\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8401 - acc: 0.2505 - val_loss: 1.8080 - val_acc: 0.3100\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8334 - acc: 0.2677 - val_loss: 1.7913 - val_acc: 0.3120\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8201 - acc: 0.2657 - val_loss: 1.7722 - val_acc: 0.3190\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8009 - acc: 0.2712 - val_loss: 1.7512 - val_acc: 0.3240\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7911 - acc: 0.2812 - val_loss: 1.7285 - val_acc: 0.3310\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7735 - acc: 0.2939 - val_loss: 1.7043 - val_acc: 0.3420\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7500 - acc: 0.3079 - val_loss: 1.6776 - val_acc: 0.3510\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7402 - acc: 0.3083 - val_loss: 1.6486 - val_acc: 0.3800\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7199 - acc: 0.3201 - val_loss: 1.6207 - val_acc: 0.3930\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6974 - acc: 0.3253 - val_loss: 1.5931 - val_acc: 0.4070\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6781 - acc: 0.3377 - val_loss: 1.5651 - val_acc: 0.4270\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6711 - acc: 0.3471 - val_loss: 1.5406 - val_acc: 0.4440\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6474 - acc: 0.3469 - val_loss: 1.5145 - val_acc: 0.4510\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6281 - acc: 0.3521 - val_loss: 1.4876 - val_acc: 0.4610\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6150 - acc: 0.3639 - val_loss: 1.4634 - val_acc: 0.4750\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5985 - acc: 0.3748 - val_loss: 1.4395 - val_acc: 0.4940\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5763 - acc: 0.3808 - val_loss: 1.4134 - val_acc: 0.5050\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5539 - acc: 0.3969 - val_loss: 1.3892 - val_acc: 0.5280\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5548 - acc: 0.3883 - val_loss: 1.3709 - val_acc: 0.5280\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5287 - acc: 0.4059 - val_loss: 1.3478 - val_acc: 0.5470\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5163 - acc: 0.4099 - val_loss: 1.3288 - val_acc: 0.5560\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5083 - acc: 0.4137 - val_loss: 1.3094 - val_acc: 0.5720\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4847 - acc: 0.4319 - val_loss: 1.2877 - val_acc: 0.5770\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4855 - acc: 0.4204 - val_loss: 1.2714 - val_acc: 0.5920\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4642 - acc: 0.4336 - val_loss: 1.2518 - val_acc: 0.6070\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4565 - acc: 0.4380 - val_loss: 1.2329 - val_acc: 0.6190\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4303 - acc: 0.4479 - val_loss: 1.2134 - val_acc: 0.6380\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4283 - acc: 0.4561 - val_loss: 1.1985 - val_acc: 0.6450\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3998 - acc: 0.4632 - val_loss: 1.1791 - val_acc: 0.6420\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3886 - acc: 0.4664 - val_loss: 1.1616 - val_acc: 0.6460\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3729 - acc: 0.4784 - val_loss: 1.1441 - val_acc: 0.6540\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3671 - acc: 0.4807 - val_loss: 1.1300 - val_acc: 0.6640\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3640 - acc: 0.4808 - val_loss: 1.1187 - val_acc: 0.6570\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3373 - acc: 0.4841 - val_loss: 1.0995 - val_acc: 0.6660\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3246 - acc: 0.4999 - val_loss: 1.0832 - val_acc: 0.6730\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3099 - acc: 0.5103 - val_loss: 1.0693 - val_acc: 0.6760\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3033 - acc: 0.5064 - val_loss: 1.0562 - val_acc: 0.6850\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2944 - acc: 0.5116 - val_loss: 1.0446 - val_acc: 0.6820\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2866 - acc: 0.5151 - val_loss: 1.0344 - val_acc: 0.6850\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2655 - acc: 0.5233 - val_loss: 1.0180 - val_acc: 0.6880\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2563 - acc: 0.5239 - val_loss: 1.0041 - val_acc: 0.6920\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2481 - acc: 0.5256 - val_loss: 0.9925 - val_acc: 0.6950\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2476 - acc: 0.5280 - val_loss: 0.9800 - val_acc: 0.6940\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2354 - acc: 0.5300 - val_loss: 0.9684 - val_acc: 0.7050\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2150 - acc: 0.5400 - val_loss: 0.9589 - val_acc: 0.7120\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2109 - acc: 0.5513 - val_loss: 0.9459 - val_acc: 0.7030\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2099 - acc: 0.5472 - val_loss: 0.9360 - val_acc: 0.7070\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1975 - acc: 0.5485 - val_loss: 0.9275 - val_acc: 0.7140\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1849 - acc: 0.5585 - val_loss: 0.9184 - val_acc: 0.7120\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1837 - acc: 0.5669 - val_loss: 0.9071 - val_acc: 0.7230\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1751 - acc: 0.5639 - val_loss: 0.8986 - val_acc: 0.7200\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1549 - acc: 0.5747 - val_loss: 0.8873 - val_acc: 0.7260\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1635 - acc: 0.5651 - val_loss: 0.8807 - val_acc: 0.7260\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1557 - acc: 0.5672 - val_loss: 0.8725 - val_acc: 0.7280\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1337 - acc: 0.5803 - val_loss: 0.8605 - val_acc: 0.7310\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1170 - acc: 0.5859 - val_loss: 0.8531 - val_acc: 0.7290\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1179 - acc: 0.5797 - val_loss: 0.8437 - val_acc: 0.7330\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1314 - acc: 0.5715 - val_loss: 0.8400 - val_acc: 0.7340\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0935 - acc: 0.5948 - val_loss: 0.8310 - val_acc: 0.7370\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0940 - acc: 0.5903 - val_loss: 0.8213 - val_acc: 0.7360\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0864 - acc: 0.5919 - val_loss: 0.8140 - val_acc: 0.7370\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0872 - acc: 0.5936 - val_loss: 0.8073 - val_acc: 0.7370\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0767 - acc: 0.5975 - val_loss: 0.7990 - val_acc: 0.7370\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0606 - acc: 0.6045 - val_loss: 0.7912 - val_acc: 0.7390\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0702 - acc: 0.6021 - val_loss: 0.7851 - val_acc: 0.7400\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0764 - acc: 0.5968 - val_loss: 0.7811 - val_acc: 0.7430\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0551 - acc: 0.6103 - val_loss: 0.7777 - val_acc: 0.7410\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0540 - acc: 0.6044 - val_loss: 0.7700 - val_acc: 0.7450\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0394 - acc: 0.6139 - val_loss: 0.7644 - val_acc: 0.7470\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0397 - acc: 0.6067 - val_loss: 0.7589 - val_acc: 0.7470\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0272 - acc: 0.6219 - val_loss: 0.7505 - val_acc: 0.7510\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0158 - acc: 0.6245 - val_loss: 0.7446 - val_acc: 0.7460\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0275 - acc: 0.6136 - val_loss: 0.7412 - val_acc: 0.7500\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0228 - acc: 0.6173 - val_loss: 0.7377 - val_acc: 0.7490\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0199 - acc: 0.6184 - val_loss: 0.7353 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0134 - acc: 0.6215 - val_loss: 0.7303 - val_acc: 0.7530\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0174 - acc: 0.6208 - val_loss: 0.7271 - val_acc: 0.7500\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0023 - acc: 0.6265 - val_loss: 0.7241 - val_acc: 0.7500\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0029 - acc: 0.6220 - val_loss: 0.7190 - val_acc: 0.7540\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9879 - acc: 0.6327 - val_loss: 0.7127 - val_acc: 0.7560\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9865 - acc: 0.6252 - val_loss: 0.7100 - val_acc: 0.7550\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9628 - acc: 0.6372 - val_loss: 0.7027 - val_acc: 0.7590\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9743 - acc: 0.6404 - val_loss: 0.7010 - val_acc: 0.7580\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9698 - acc: 0.6383 - val_loss: 0.6961 - val_acc: 0.7580\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9758 - acc: 0.6328 - val_loss: 0.6926 - val_acc: 0.7590\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9662 - acc: 0.6391 - val_loss: 0.6881 - val_acc: 0.7580\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9672 - acc: 0.6409 - val_loss: 0.6854 - val_acc: 0.7550\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9480 - acc: 0.6501 - val_loss: 0.6818 - val_acc: 0.7640\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9476 - acc: 0.6475 - val_loss: 0.6776 - val_acc: 0.7610\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9526 - acc: 0.6485 - val_loss: 0.6744 - val_acc: 0.7600\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9478 - acc: 0.6456 - val_loss: 0.6734 - val_acc: 0.7620\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9373 - acc: 0.6532 - val_loss: 0.6690 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9404 - acc: 0.6469 - val_loss: 0.6653 - val_acc: 0.7620\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9366 - acc: 0.6524 - val_loss: 0.6629 - val_acc: 0.7620\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9141 - acc: 0.6569 - val_loss: 0.6577 - val_acc: 0.7650\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9396 - acc: 0.6484 - val_loss: 0.6591 - val_acc: 0.7620\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9192 - acc: 0.6569 - val_loss: 0.6553 - val_acc: 0.7630\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9250 - acc: 0.6605 - val_loss: 0.6542 - val_acc: 0.7620\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9224 - acc: 0.6533 - val_loss: 0.6496 - val_acc: 0.7640\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9200 - acc: 0.6568 - val_loss: 0.6510 - val_acc: 0.7640\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9003 - acc: 0.6680 - val_loss: 0.6470 - val_acc: 0.7610\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9010 - acc: 0.6671 - val_loss: 0.6434 - val_acc: 0.7660\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9034 - acc: 0.6628 - val_loss: 0.6428 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9110 - acc: 0.6584 - val_loss: 0.6407 - val_acc: 0.7640\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8934 - acc: 0.6695 - val_loss: 0.6380 - val_acc: 0.7670\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8921 - acc: 0.6660 - val_loss: 0.6374 - val_acc: 0.7640\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8894 - acc: 0.6664 - val_loss: 0.6322 - val_acc: 0.7660\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8856 - acc: 0.6701 - val_loss: 0.6285 - val_acc: 0.7670\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8889 - acc: 0.6688 - val_loss: 0.6283 - val_acc: 0.7700\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8769 - acc: 0.6671 - val_loss: 0.6276 - val_acc: 0.7680\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8794 - acc: 0.6720 - val_loss: 0.6248 - val_acc: 0.7670\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8593 - acc: 0.6759 - val_loss: 0.6234 - val_acc: 0.7690\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8801 - acc: 0.6691 - val_loss: 0.6223 - val_acc: 0.7690\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8658 - acc: 0.6731 - val_loss: 0.6190 - val_acc: 0.7720\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8869 - acc: 0.6708 - val_loss: 0.6189 - val_acc: 0.7750\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8563 - acc: 0.6824 - val_loss: 0.6160 - val_acc: 0.7750\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8640 - acc: 0.6839 - val_loss: 0.6151 - val_acc: 0.7740\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8484 - acc: 0.6855 - val_loss: 0.6113 - val_acc: 0.7710\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8609 - acc: 0.6819 - val_loss: 0.6112 - val_acc: 0.7730\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8558 - acc: 0.6849 - val_loss: 0.6074 - val_acc: 0.7730\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8475 - acc: 0.6873 - val_loss: 0.6076 - val_acc: 0.7700\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8470 - acc: 0.6843 - val_loss: 0.6069 - val_acc: 0.7720\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8506 - acc: 0.6827 - val_loss: 0.6039 - val_acc: 0.7750\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8442 - acc: 0.6788 - val_loss: 0.6027 - val_acc: 0.7720\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8418 - acc: 0.6872 - val_loss: 0.6037 - val_acc: 0.7760\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8406 - acc: 0.6839 - val_loss: 0.6015 - val_acc: 0.7760\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8358 - acc: 0.6884 - val_loss: 0.5979 - val_acc: 0.7790\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8416 - acc: 0.6879 - val_loss: 0.5957 - val_acc: 0.7770\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8329 - acc: 0.6801 - val_loss: 0.5942 - val_acc: 0.7750\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8179 - acc: 0.6956 - val_loss: 0.5947 - val_acc: 0.7760\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8390 - acc: 0.6831 - val_loss: 0.5946 - val_acc: 0.7770\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8243 - acc: 0.6896 - val_loss: 0.5922 - val_acc: 0.7760\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8181 - acc: 0.6913 - val_loss: 0.5918 - val_acc: 0.7810\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8336 - acc: 0.6841 - val_loss: 0.5914 - val_acc: 0.7810\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8093 - acc: 0.6941 - val_loss: 0.5911 - val_acc: 0.7800\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.6928 - val_loss: 0.5864 - val_acc: 0.7850\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8137 - acc: 0.7011 - val_loss: 0.5850 - val_acc: 0.7760\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8236 - acc: 0.6940 - val_loss: 0.5852 - val_acc: 0.7820\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8231 - acc: 0.6897 - val_loss: 0.5852 - val_acc: 0.7830\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8060 - acc: 0.7013 - val_loss: 0.5845 - val_acc: 0.7790\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7927 - acc: 0.7033 - val_loss: 0.5826 - val_acc: 0.7810\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7906 - acc: 0.7007 - val_loss: 0.5800 - val_acc: 0.7840\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8047 - acc: 0.6956 - val_loss: 0.5789 - val_acc: 0.7820\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8041 - acc: 0.7039 - val_loss: 0.5785 - val_acc: 0.7840\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7914 - acc: 0.7107 - val_loss: 0.5778 - val_acc: 0.7770\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7874 - acc: 0.7096 - val_loss: 0.5766 - val_acc: 0.7780\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7860 - acc: 0.7115 - val_loss: 0.5753 - val_acc: 0.7880\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7962 - acc: 0.7027 - val_loss: 0.5772 - val_acc: 0.7800\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7840 - acc: 0.7115 - val_loss: 0.5736 - val_acc: 0.7850\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7821 - acc: 0.7073 - val_loss: 0.5730 - val_acc: 0.7820\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7884 - acc: 0.7077 - val_loss: 0.5700 - val_acc: 0.7840\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7944 - acc: 0.7095 - val_loss: 0.5711 - val_acc: 0.7820\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7728 - acc: 0.7117 - val_loss: 0.5682 - val_acc: 0.7810\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7821 - acc: 0.7048 - val_loss: 0.5666 - val_acc: 0.7840\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7830 - acc: 0.7076 - val_loss: 0.5673 - val_acc: 0.7840\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7835 - acc: 0.7121 - val_loss: 0.5684 - val_acc: 0.7830\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7745 - acc: 0.7136 - val_loss: 0.5668 - val_acc: 0.7850\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7770 - acc: 0.7079 - val_loss: 0.5686 - val_acc: 0.7810\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7860 - acc: 0.7053 - val_loss: 0.5658 - val_acc: 0.7860\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7738 - acc: 0.7079 - val_loss: 0.5644 - val_acc: 0.7880\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7666 - acc: 0.7068 - val_loss: 0.5638 - val_acc: 0.7820\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7707 - acc: 0.7079 - val_loss: 0.5627 - val_acc: 0.7840\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7840 - acc: 0.7064 - val_loss: 0.5618 - val_acc: 0.7870\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7650 - acc: 0.7151 - val_loss: 0.5598 - val_acc: 0.7850\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7661 - acc: 0.7088 - val_loss: 0.5599 - val_acc: 0.7860\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7444 - acc: 0.7239 - val_loss: 0.5568 - val_acc: 0.7870\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7523 - acc: 0.7149 - val_loss: 0.5578 - val_acc: 0.7830\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7616 - acc: 0.7185 - val_loss: 0.5593 - val_acc: 0.7850\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7566 - acc: 0.7108 - val_loss: 0.5588 - val_acc: 0.7860\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7467 - acc: 0.7197 - val_loss: 0.5590 - val_acc: 0.7830\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7537 - acc: 0.7157 - val_loss: 0.5566 - val_acc: 0.7800\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7494 - acc: 0.7199 - val_loss: 0.5566 - val_acc: 0.7830\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7526 - acc: 0.7203 - val_loss: 0.5554 - val_acc: 0.7880\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7362 - acc: 0.7256 - val_loss: 0.5528 - val_acc: 0.7860\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7464 - acc: 0.7181 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7515 - acc: 0.7141 - val_loss: 0.5511 - val_acc: 0.7850\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7432 - acc: 0.7191 - val_loss: 0.5511 - val_acc: 0.7880\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7388 - acc: 0.7232 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7412 - acc: 0.7212 - val_loss: 0.5543 - val_acc: 0.7840\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7288 - acc: 0.7279 - val_loss: 0.5510 - val_acc: 0.7830\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7335 - acc: 0.7285 - val_loss: 0.5509 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7255 - acc: 0.7292 - val_loss: 0.5480 - val_acc: 0.7840\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7341 - acc: 0.7236 - val_loss: 0.5490 - val_acc: 0.7830\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7398 - acc: 0.7244 - val_loss: 0.5489 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9131 - acc: 0.1977 - val_loss: 1.8734 - val_acc: 0.2517\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.8204 - acc: 0.3034 - val_loss: 1.7551 - val_acc: 0.3397\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.6686 - acc: 0.4072 - val_loss: 1.5741 - val_acc: 0.4647\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.4662 - acc: 0.5248 - val_loss: 1.3619 - val_acc: 0.5560\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.2557 - acc: 0.6060 - val_loss: 1.1666 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0768 - acc: 0.6660 - val_loss: 1.0120 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.9451 - acc: 0.7012 - val_loss: 0.9037 - val_acc: 0.7047\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.8536 - acc: 0.7191 - val_loss: 0.8281 - val_acc: 0.7210\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7894 - acc: 0.7321 - val_loss: 0.7750 - val_acc: 0.7300\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7434 - acc: 0.7419 - val_loss: 0.7363 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7086 - acc: 0.7492 - val_loss: 0.7075 - val_acc: 0.7450\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6815 - acc: 0.7555 - val_loss: 0.6846 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6596 - acc: 0.7612 - val_loss: 0.6661 - val_acc: 0.7587\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6413 - acc: 0.7678 - val_loss: 0.6501 - val_acc: 0.7623\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6259 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6119 - acc: 0.7780 - val_loss: 0.6275 - val_acc: 0.7700\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5999 - acc: 0.7809 - val_loss: 0.6168 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5891 - acc: 0.7845 - val_loss: 0.6079 - val_acc: 0.7757\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5792 - acc: 0.7892 - val_loss: 0.6013 - val_acc: 0.7753\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5700 - acc: 0.7920 - val_loss: 0.5924 - val_acc: 0.7813\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5614 - acc: 0.7954 - val_loss: 0.5879 - val_acc: 0.7807\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5535 - acc: 0.7989 - val_loss: 0.5832 - val_acc: 0.7817\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5391 - acc: 0.8044 - val_loss: 0.5735 - val_acc: 0.7850\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5326 - acc: 0.8076 - val_loss: 0.5674 - val_acc: 0.7937\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8092 - val_loss: 0.5622 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5198 - acc: 0.8114 - val_loss: 0.5599 - val_acc: 0.7977\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5144 - acc: 0.8140 - val_loss: 0.5571 - val_acc: 0.8000\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5087 - acc: 0.8162 - val_loss: 0.5509 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5033 - acc: 0.8180 - val_loss: 0.5483 - val_acc: 0.8020\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4982 - acc: 0.8205 - val_loss: 0.5443 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4934 - acc: 0.8222 - val_loss: 0.5435 - val_acc: 0.8027\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4885 - acc: 0.8253 - val_loss: 0.5426 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4840 - acc: 0.8256 - val_loss: 0.5386 - val_acc: 0.8080\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4799 - acc: 0.8278 - val_loss: 0.5341 - val_acc: 0.8093\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4755 - acc: 0.8305 - val_loss: 0.5322 - val_acc: 0.8100\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4713 - acc: 0.8308 - val_loss: 0.5297 - val_acc: 0.8117\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4674 - acc: 0.8319 - val_loss: 0.5273 - val_acc: 0.8123\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4632 - acc: 0.8339 - val_loss: 0.5265 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8355 - val_loss: 0.5236 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8370 - val_loss: 0.5241 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4522 - acc: 0.8383 - val_loss: 0.5210 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4487 - acc: 0.8403 - val_loss: 0.5223 - val_acc: 0.8143\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4453 - acc: 0.8405 - val_loss: 0.5187 - val_acc: 0.8180\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4420 - acc: 0.8427 - val_loss: 0.5206 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8437 - val_loss: 0.5186 - val_acc: 0.8120\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4358 - acc: 0.8442 - val_loss: 0.5154 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4324 - acc: 0.8458 - val_loss: 0.5156 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4296 - acc: 0.8474 - val_loss: 0.5147 - val_acc: 0.8150\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8484 - val_loss: 0.5136 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8492 - val_loss: 0.5142 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5135 - val_acc: 0.8133\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4181 - acc: 0.8508 - val_loss: 0.5154 - val_acc: 0.8163\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4152 - acc: 0.8521 - val_loss: 0.5109 - val_acc: 0.8140\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.5124 - val_acc: 0.8160\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8537 - val_loss: 0.5126 - val_acc: 0.8163\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8543 - val_loss: 0.5112 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4052 - acc: 0.8555 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8556 - val_loss: 0.5111 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4000 - acc: 0.8580 - val_loss: 0.5105 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3977 - acc: 0.8590 - val_loss: 0.5116 - val_acc: 0.8163\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8602 - val_loss: 0.5123 - val_acc: 0.8180\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8606 - val_loss: 0.5089 - val_acc: 0.8157\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8617 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3888 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8633 - val_loss: 0.5122 - val_acc: 0.8167\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8638 - val_loss: 0.5100 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3821 - acc: 0.8646 - val_loss: 0.5113 - val_acc: 0.8160\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5136 - val_acc: 0.8120\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8661 - val_loss: 0.5121 - val_acc: 0.8163\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8685 - val_loss: 0.5115 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8689 - val_loss: 0.5121 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5157 - val_acc: 0.8160\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8702 - val_loss: 0.5137 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8725 - val_loss: 0.5126 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8718 - val_loss: 0.5138 - val_acc: 0.8147\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3627 - acc: 0.8725 - val_loss: 0.5194 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3610 - acc: 0.8739 - val_loss: 0.5152 - val_acc: 0.8117\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3589 - acc: 0.8742 - val_loss: 0.5166 - val_acc: 0.8170\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3571 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8765 - val_loss: 0.5159 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3536 - acc: 0.8771 - val_loss: 0.5180 - val_acc: 0.8157\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8768 - val_loss: 0.5189 - val_acc: 0.8140\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3501 - acc: 0.8779 - val_loss: 0.5177 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8785 - val_loss: 0.5218 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5212 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3456 - acc: 0.8793 - val_loss: 0.5198 - val_acc: 0.8153\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3438 - acc: 0.8801 - val_loss: 0.5210 - val_acc: 0.8143\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8802 - val_loss: 0.5235 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3406 - acc: 0.8813 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8818 - val_loss: 0.5223 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3374 - acc: 0.8827 - val_loss: 0.5232 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5240 - val_acc: 0.8150\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8832 - val_loss: 0.5284 - val_acc: 0.8160\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3328 - acc: 0.8852 - val_loss: 0.5263 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.8856 - val_loss: 0.5260 - val_acc: 0.8137\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8863 - val_loss: 0.5297 - val_acc: 0.8140\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3257 - acc: 0.8878 - val_loss: 0.5295 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3239 - acc: 0.8889 - val_loss: 0.5335 - val_acc: 0.8143\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8888 - val_loss: 0.5320 - val_acc: 0.8153\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3212 - acc: 0.8890 - val_loss: 0.5335 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8891 - val_loss: 0.5339 - val_acc: 0.8150\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8903 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3171 - acc: 0.8912 - val_loss: 0.5352 - val_acc: 0.8147\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5379 - val_acc: 0.8127\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.5363 - val_acc: 0.8137\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5379 - val_acc: 0.8133\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8927 - val_loss: 0.5388 - val_acc: 0.8153\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8924 - val_loss: 0.5392 - val_acc: 0.8147\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8947 - val_loss: 0.5406 - val_acc: 0.8137\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8943 - val_loss: 0.5422 - val_acc: 0.8157\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8951 - val_loss: 0.5433 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8956 - val_loss: 0.5432 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.5483 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8965 - val_loss: 0.5461 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.5459 - val_acc: 0.8127\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8972 - val_loss: 0.5460 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 21us/step\n",
      "4000/4000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29492314792401864, 0.8997272727272727]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
